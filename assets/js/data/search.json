[ { "title": "Azure DevOps Commit Message Validator and PR Linker GitHub Action", "url": "/posts/azdo-commit-message-validator-and-pr-linker-github-action/", "categories": "GitHub, Actions", "tags": "Azure DevOps, Work Items, GitHub, GitHub Actions, Pull Requests, Branch Protection Rules", "date": "2022-08-17 13:00:00 -0500", "snippet": "OverviewI was with a client recently that was using GitHub for source control and GitHub Advanced Security, and Azure DevOps for Boards and Pipelines. Integrating GitHub with Azure DevOps is relatively simple for linking commits and pull requests, but there were a few pieces that we wanted to improve on. One was making sure / enforcing in the pull request that each commit contains an Azure Boards work item link with AB#123 in the commit message. We also found that commits that contained work item links weren’t automatically linked to the pull request. The pull request needs to contain AB#123 in the pull request title or body in order for the link to be automatically created.Because of these limitations, I built an action to be ran in a pull request to make sure that all commits have a AB#123 link in the commit message, as well as link all corresponding work items to the pull request.Using the ActionThe action loops through each commit and: makes sure it has AB#123 in the commit message if yes, add a GitHub Pull Request link to the work item in Azure DevOpsPrerequisites Create a repository secret titled AZURE_DEVOPS_PAT - it needs to be a full PAT Pass the Azure DevOps organization to the azure-devops-organization input parameter (line no. 14 below)YMLThis should only be triggered via pull requests.name: pr-commit-message-enforcer-and-linkeron: pull_request: branches: [ \"main\" ]jobs: pr-commit-message-enforcer-and-linker: runs-on: ubuntu-latest steps: - uses: actions/checkout@v3 - name: Azure DevOps Commit Validator and Pull Request Linker uses: joshjohanning/azdo_commit_message_validator@v1 with: azure-devops-organization: myorg # The name of the Azure DevOps organization azure-devops-token: ${{ secrets.AZURE_DEVOPS_PAT }} # \"Azure DevOps Personal Access Token (needs to be a full PAT) fail-if-missing-workitem-commit-link: true # Fail the action if a commit in the pull request is missing AB# in the commit message link-commits-to-pull-request: true # Link the work items found in commits to the pull requestBranch Protection PolicyAfter you create the workflow, you can add this as a status check to the branch protection policy on your default branch. If you aren’t seeing the pr-commit-message-enforcer-and-linker job name, you might have to create a pull request that triggers the job first and then add the branch protection policy.Configuring the status check in the branch protection policyOnce added, if commit message(s) don’t contain an AB#123 link, the pull request will be blocked from merging.The status checks on the pull request are failing because of missing work item links in the commit message(s)ScreenshotsIf a commit in the pull request is missing AB# in the commit message, the action will fail:Blocking the pull request because it’s missing work item linksThe action will link all work items found in commits to the pull request:Linking the work items to the pull requestThe pull request showing along with the commit on the work item in Azure DevOps:Pull request link on a work item in Azure DevOpsSummaryThe gist is that it makes sure that all commits in the pull request have an AB# link in the commit message, and that all work items found in the commits are linked to the pull request. I’m working with an undocumented API that I describe a bit more in the README of the repository if you’re interested. Test it out - feedback’s always welcome!" }, { "title": "GitHub: Script to Bulk Add Users to a Team", "url": "/posts/github-script-to-add-users-to-teams/", "categories": "GitHub", "tags": "GitHub, Scripts, gh cli", "date": "2022-08-11 16:00:00 -0500", "snippet": "OverviewIf you’ve ever had to add several users to a team in a GitHub organization, you know it can be a pain as it’s one user at a time and multiple clicks per add. This script aims to simplify that by adding users to a GitHub org team programmatically from a CSV file.Using the ScriptScript link in my github-misc-scripts repo0. Prerequisite: You need to make sure to have the gh cli installed and authorized (gh auth login).1. Creates a users.csv with the list of users to add to the team, one per line, and leave a trailing empty line/whitespace at the end of the file. The file should look like:user1user22. From there, it’s pretty simple - run the script:./add-users-to-team-from-list.sh users.csv &lt;org&gt; &lt;team&gt;SummaryHopefully this saves you some time in the UI when adding multiple users to a team!" }, { "title": "A Lap Around GitHub Advanced Security (30m Video)", "url": "/posts/lap-around-github-advanced-security/", "categories": "GitHub, Advanced Security", "tags": "GitHub, Dependabot, GitHub Actions, GitHub Advanced Security, Branch Protection Rules, CodeQL, Policy Enforcement, Pull Requests", "date": "2022-08-03 21:00:00 -0500", "snippet": "OverviewI realized that there wasn’t any content of me speaking on the internet, just blogging, so I thought I would at least create one! This is a video I created to explain the features of GitHub Advanced Security (GHAS), features, and some tips and tricks for configuring and interpreting the results along the way.I admittedly didn’t put a ton of polish around the video, but I hope it’s useful as a primer if you are new to GitHub Advanced Security, or even if you’re not new, perhaps you’ll learn a few new things along the way.Enjoy!" }, { "title": "Configure GitHub Dependabot to Keep Actions Up to Date", "url": "/posts/github-dependabot-for-actions/", "categories": "GitHub, Dependabot", "tags": "GitHub, Dependabot, Pull Requests, GitHub Actions", "date": "2022-07-02 08:00:00 -0500", "snippet": "OverviewYou probably know that Dependabot can be used to update your packages, such as NPM or NuGet, but did you also know you can use it to keep Actions up to date in your GitHub Actions Workflow?What about for custom Actions that you have create in your organization, did you know you can use Dependabot to keep those up to date as well?I will show you how to do this both for Actions in the public marketplace and custom actions you have created in your organization internally.Marketplace ActionsConfiguring Dependabot with marketplace actions is pretty easy. We’re using the Dependabot Version Updates functionality, so we have to create our dependabot.yml file manually. There are 3 ways to do this: Under the repository Settings page &gt; Code security and analysis &gt; Dependabot version updates, you can click the Configure button to prepopulate the dependabot.yml file. Under the repository Insights page &gt; Dependency Graph &gt; Dependabot &gt; Create Config File Create your own file in the .github/dependabot.yml directory.Whichever one you pick, you will still have to configure the dependabot.yml file with which package ecosystems you want it to pick up.For GitHub Actions in the marketplace, it would look like this:version: 2updates: # Maintain dependencies for GitHub Actions - package-ecosystem: \"github-actions\" # Workflow files stored in the default location of `.github/workflows` directory: \"/\" schedule: interval: \"daily\" open-pull-requests-limit: 10Note that even though your workflows are in the .github/workflows directory, Dependabot still expects the directory on line 8 to be set to \"/\" (documented here).I also like to set open-pull-requests-limit explicitly, otherwise the default maximum number of pull requests that will be created per package ecosystem defined is 5.Custom Actions in OrganizationSo far, this is pretty well documented. But what is a little harder to figure out is how to use this for custom actions in private/internal repositories within an organization. There are two different ways to do this, and I will talk about both.The first doesn’t require any different configuration than the above. However, when you use a custom action, you will see an error in Dependabot:Dependabot throws an error and requests you to grant accessYou would have to grant access to every custom action repository in your organization - which seems untenable.You can proactively add the private action repositories via Organization Settings &gt; Code security and analysis &gt; Grant Dependabot access to private repositories, but again, this seems less than ideal, especially since I don’t think there’s an API or GraphQL method of updating this.Granting Dependabot access to private repos in organization settingsThe second way to do this is to use a Dependabot secret (GitHub PAT) and a git repository as a private registry.Here’s the dependabot.yml file:version: 2updates: # Maintain dependencies for GitHub Actions - package-ecosystem: \"github-actions\" # Workflow files stored in the default location of `.github/workflows` directory: \"/\" schedule: interval: \"daily\" registries: - githubregistries: github: type: git url: https://github.com username: x-access-token # username doesn't matter password: ${{ secrets.GHEC_TOKEN }}You’ll notice the password: ${{ secrets.GHEC_TOKEN }} on line 17. We need to create a Dependabot Secret using a GitHub Personal Access Token (PAT). I know that managing a PAT can be annoying and potentially insecure, but on the upside, Dependabot Secrets can only be accessed via Dependabot, and the Dependabot implementation is essentially a black box to us. They can’t be accessed maliciously / inappropriately through GitHub Actions workflow runs.What I recommend is: Creating a machine user or service account that only has read-access to the repositories in the organization - note that this will consume a license Log into that account and create a PAT that doesn’t expire with only the repositories scope selected Create an organization secret for Dependabot - this way all the repositories in the organization will be able to access the Dependabot secretNotes: In theory you could create the PAT under anyone’s account since no one can dump the PAT from a GitHub Action workflow and it would be fine, but I think it’s better to have it under a machine user or service account so that Dependabot will still work if / when that original person is no longer with the company Another reason to use a machine user or service account is that you can’t currently create a PAT with a read only repo scope - it’s all or nothing And before you ask: you unfortunately can’t use a GitHub App here, at least not with the native Dependabot implementationOnce you configure the dependabot.yml and Dependabot secret as discussed above, the next time Dependabot runs, it will create pull requests for you for both marketplace AND private / custom actions.Dependabot created pull requests for both marketplace and private / custom actionsSummaryKeeping marketplace actions up to date is one thing, but keeping your custom actions might be just as important! With the magic of Dependabot, you can keep your custom actions up to date without having to manually check for updates." }, { "title": "My macOS Development Environment: iTerm2, oh-my-zsh, and VS Code", "url": "/posts/my-macos-development-environment/", "categories": "", "tags": "VS Code", "date": "2022-07-01 14:30:00 -0500", "snippet": "OverviewA new team member had just joined my team at GitHub and it was their first time using macOS as the primary work machine. They had asked if I had any tips on setting up your local development environment. Hint: I do! I also came from a Windows background and only first started using macOS for work in late 2019.I was going to link them to my Powerlevel10k Zsh Theme in GitHub Codespaces, but then I realized: this is for setting up a development environment in Codespaces, not so much locally. I wrote up these instructions for my co-worker, but I thought I would re-purpose them into a blog post that I can share with others as well!iTerm2, oh-my-zsh, and powerlevel10k theme setup Install iTerm2: brew install --cask iterm2 Install the MesloLGS fonts Download my iTerm profile as a json file and import into iTerm In iTerm, go to: Preferences &gt; Profile, you can use the + to import the iterm2-profile.json profile I believe the only other special things that I have in the profile (other than colors) is the ability to use OPTION+arrow keys to to go left / right to the end of strings, OPTION+SHIFT+arrow keys to highlight entire strings, and OPTION+Backspace to delete an entire strings Install oh-my-zsh (run the curl command) Install plugins like zsh-autosuggestions, zsh-syntax-highlighting (basically you clone the repo and then add the plugin to the list of plugins in your ~/.zshrc file Install powerlevel10k zsh theme - basically clone the repo and modify the ~/.zshrc file to update the ZSH_THEME You will be prompted to configure powerlevel10k - but my configuration for ~/.p10k.zsh is here My ~/.zshrc config is here Make iTerm2 the default terminal: Make iTerm default terminal (^ + Shift + Command + \\)That should be all you need to make your terminal look exactly like mine 😀.My iTerm terminalIf you’re using the powerlevel10k theme, make sure to set up the font in VS Code’s terminal as well! You can now back up your ~/.zshrc file and ~/.p10k.zsh files in a dotfiles repository similar to mine by creating symlinks (documentation on how to do this is in my repo also).VS CodeTerminalTo allow VS Code’s terminal to look similar to the iTerm terminal, there are a few additional things we need. Add/modify these lines to your VS Code settings.json file by opening the command palette (CMD/CTRL + Shift + P) and typing &gt; Preferences: Open Settings (JSON) : Adding the font configuration, osx shell, and shell to use when using a Linux environment (ie: in Codespaces):{ \"terminal.integrated.shell.osx\": \"/bin/zsh\", \"terminal.integrated.defaultProfile.linux\": \"zsh\", \"terminal.integrated.fontFamily\": \"MesloLGS NF\",}Now the terminal in VS Code looks nice also!The terminal looks good in VS Code too! Pro-tip: Turn on VS Code settings sync!ExtensionsI’ll just highlight some of my favorite extensions that I use in VS Code: GitHub Copilot - Because of course Markdown All in One - I love this because I can highlight a piece of text and paste in a link and it will automatically format the markdown for me, similar to this feature in GitHub Code Spell Checker - to help me from misspelling, and as a bonus if you’re using VS Code settings sync, you can keep a custom dictionary synced across VS Code instances / Codespaces by using the “quick fix” on aka ⌘ + . on unrecognized words and “add to user settings” YAML - for YAML syntax highlighting in the editor Bracket Pair Colorizer - I used to use Bracket Pair Colorizer 2, but this is now built-in to VS Code by adding this to your settings: \"editor.guides.bracketPairs\": true Draw.io Integration - for creating charts/architecture diagrams directly in VS Code GitLense - it’s nice to be able to do things such as opening up a Git Blame view inline like you can in GitHubKey BindingsComing from Windows, my brain is wired that CTRL + Z is undo and CTRL + Y is redo. In macOS, undo is ⌘ + Z and redo is ⌘ + SHIFT + Z. I added this key binding to allow for both ⌘ + SHIFT + Z AND ⌘ + Y to be used for redo while editing in VS Code.You can modify VS Code’s keybindings.json file by opening the command palette (CMD/CTRL + Shift + P) and typing &gt; Preferences: Open Keyboard Shortcuts (JSON):{ \"keybindings\": [ { \"key\": \"ctrl+z\", \"command\": \"undo\", \"when\": \"editorTextFocus\" }, { \"key\": \"ctrl+shift+z\", \"command\": \"redo\", \"when\": \"editorTextFocus\" } ]}BrewMy Brewfile of things that I have installed with Brew is here.You can install everything in the Brewfile by running:brew bundle install --file=./BrewfileI was able to generate the Brewfile by running:brew bundle dumpApp Store AppsThese are my must have App Store apps: Magnet ($$$) - for pinning windows to certain regions of the screen Copyclip (free) - for clipboard management I like to go into preferences and remember and display 2,000 clippings and start at system startup! Get Plain Text (free) - paste without formatting I set my keyboard shortcut to ⌥ ⌘ V as well as launching at startup Troubleshooting Question: I’m seeing special characters that aren’t rendering in my terminal? Answer: Make sure you install the MesloLGS fonts and configure it to be used in iTerm and VS Code. Powerlevel10k uses custom glyphs from the font to render the terminal correctly.SummaryBefore writing this post, I had most of this in OneNote of what to do when I get a new Mac. Most things are automated, but some like the App Store Apps I install, are not. I plan on sharing this with folks who ask how to get started quickly on a new Mac!Let me know anything I missed or improvements I can make here, or tips for anyone else coming over from the Windows world 🙏" }, { "title": "Configure actions-runner-controller without cert-manager", "url": "/posts/actions-runner-controller-without-cert-manager/", "categories": "GitHub, Actions", "tags": "GitHub, GitHub Actions, actions-runner-controller", "date": "2022-06-28 16:00:00 -0500", "snippet": "Overviewactions-runner-controller is a great way to set up self-scaling GitHub runners in a Kubernetes cluster. This allows teams to scale up their self-hosted runners as more jobs are queued throughout the day and scale down at night when there are fewer jobs running. There is a lot of documentation to digest in the repository, but for the most part, it’s relatively easy to get started with some basic scaling. The only prerequisite (other than having access to the Kubernetes cluster and administrative access to GitHub repo or organization) is cert-manager.However, sometimes organizations have their own certificate requirements, and prefer to manage their own certificates vs. letting a tool like cert-manager manage them. This is where the current documentation is lacking and unclear. Other people have asked the same question, as well as my most recent customer, and we were able to figure it out so I’ll document it here!Configuring without cert-managerI’m going to be creating self-signed certificates, but you could imagine this working with certificates provided to you from the security team. If creating your own certificates, you will need openssl installed and callable via the command line.1. Create RSA keys for CA cert and Server cert - this will output ca.key and server.keyopenssl genrsa -out ca.key 4096openssl genrsa -out server.key 40962. Create a CA configuration file (ca.conf) - the basicConstraints and keyUsage sections are required in order for the CA to be able to sign certificatesbasicConstraints = CA:TRUEkeyUsage = cRLSign, keyCertSign[req]distinguished_name = req_distinguished_nameprompt = no[req_distinguished_name]C = USST = SomeStateL = SomeCityO = SomeOrgemailAddress = your@email.comCN = actionrunners.yourorg.com3. Create the CA certificate with the ca.conf file - this will output ca.crtopenssl req -x509 -new -sha512 -nodes -key ./ca.key -days 7307 -out ./ca.crt -config ./ca.conf4. Optionally validate that the CA certificate created successfullyopenssl x509 -noout -text -in ./ca.crt5. Create your Server certificate config file - ie server.cnf All 3 SANs (alt_names) are needed For the 3rd alt_name, the actions-runner-system is the namespace - if you are installing into a different namespace, replace actions-runner-system with the namespace you are installing to (ie: default)[req]default_bits = 4096prompt = nodefault_md = sha256x509_extensions = v3_reqdistinguished_name = dn[dn]C = USST = SomeStateL = SomeCityO = SomeOrgemailAddress = your@email.comCN = actionrunners.yourorg.com[v3_req]subjectAltName = @alt_names[alt_names]DNS.1 = webhook-service.actions-runner-system.svcDNS.2 = webhook-service.actions-runner-system.svc.cluster.localDNS.3 = actions-runner-controller-webhook.actions-runner-system.svc6. Create the server Certificate Signing Request (csr) - this will outut server.csropenssl req -new -key ./server.key -out ./server.csr -config ./server.conf7. Create your Server certificate - this will output server.crtopenssl x509 -req -in ./server.csr -CA ./ca.crt -CAkey ./ca.key \\ -CAcreateserial -out ./server.crt -days 10000 \\ -extensions v3_req -extfile ./server.conf8. Optionally inspect your Server cert to make sure it has the Subject Alternate Names (SANs, aka the alt_names from step #5)openssl x509 -noout -text -in ./server.crt9. Base64 the CA cert and copy to clipboard (pbcopy is a macOS command, if running elsewhere just echo $CA_BUNDLE and copy)CA_BUNDLE=$(cat ca.crt | base64)echo $CA_BUNDLE | pbcopy10. Set the admissionWebHooks.caBundle value in the values.yaml file to the base64 value of the CA cert - you may have to remove the extra {} under admissionWebHooksadmissionWebHooks: # {} # need to remove this caBundle: \"Ci0tL...\"11. In the values.yaml file, ensure certManagerEnabled is set to falsecertManagerEnabled: false12. Create your certificate secrets using kubectl - both of these are neededkubectl create secret tls webhook-server-cert -n actions-runner-system \\ --cert=./server.crt --key=./server.keykubectl create secret tls actions-runner-controller-serving-cert -n actions-runner-system \\ --cert=./server.crt --key=./server.key13. Run the helm upgrade command to install the controllerhelm upgrade --install --namespace actions-runner-system --create-namespace \\ --wait actions-runner-controller actions-runner-controller/actions-runner-controller \\ --values ./values.yamlNote: Make sure to you have ran the helm repo add command already14. Ensure that your actions-runner-controller pod in the actions-runner-system namespace has started - if it fails, describe the pod and check the events15. Deploy your runner configuration - my example with org runners and metric-based scalingkubectl apply -f runner.yaml --namespace default16. Ensure that your runner pods have started (describe them if not); check GitHub to see if your runner(s) show there alsoHere’s me running k9s to visualize the pods and ensure they are running:The controller running in the actions-runner-system namespace; the runner pods running in the default namespaceCorresponding runners show up as org runners in GitHub:Runners with the same name as the pod name show up as org runners in GitHubSummaryactions-runner-controller is great, but sometimes you need some good trial-and-error to do things off the beaten path. Luckily, we were able to figure out how to configure the controller without using cert-manager by interpreting the errors we saw when describing the failing pods.As an enhancement, I could see you wanting a separate certificate for the actions-runner-controller-serving-cert and webhook-server-cert in step #12 - but I’ll leave further certificate optimizations to the certificate pros.Feel free to ask any questions or share any enhancements!" }, { "title": "Delete GitHub Branch Protection Rules Programmatically", "url": "/posts/github-delete-branch-protection-rules/", "categories": "GitHub", "tags": "GitHub, Branch Protection Rules, Scripts", "date": "2022-05-27 12:00:00 -0500", "snippet": "OverviewAfter a migration, or maybe when doing cleanup, you may want to delete branch protection rules in bulk. Instead of having to click through each branch protection rule individually, I wrote a PowerShell script that leverages the GraphQL endpoint. At the time I wrote this a few years ago, there wasn’t an API for deleting branch protection rules, only GraphQL. However, there is now an API endpoint for managing branch protection rules, so if I were to re-write this, that’s likely what I would use.But this script works just fine as is to delete branch protection rules programmatically, and I thought it was time to share it!ScriptThe script is also located here.############################################################### Delete branch protection rules##############################################################[CmdletBinding()]param ( [parameter (Mandatory = $true)][string]$PersonalAccessToken, [parameter (Mandatory = $true)][string]$GitHubOrg, [parameter (Mandatory = $true)][string]$GitHubRepo, [parameter (Mandatory = $true)][string]$PatternToDelete # If you want to delete branch protection rules that start with \"feature\", pass in \"feature*\" # If you want to delete ALL branch protection rules, pass in \"*\")# Example that deletes ALL branch protection rules:# ./github-delete-branch-protection.ps1 -PersonalAccessToken \"xxx\" -GitHubOrg \"myorg\" -GitHubRepo \"myrepo\" -PatternToDelete \"*\"# Example that deletes branch protection rules that start with feature:# ./github-delete-branch-protection.ps1 -PersonalAccessToken \"xxx\" -GitHubOrg \"myorg\" -GitHubRepo \"myrepo\" -PatternToDelete \"feature*\"# Set up API Header$AuthenticationHeader = @{Authorization = 'Basic ' + [Convert]::ToBase64String([Text.Encoding]::ASCII.GetBytes(\":$($PersonalAccessToken)\")) }####### Script ########## GRAPHQL# Get body$body = @{ query = \"query BranchProtectionRule { repository(owner:`\"$GitHubOrg`\", name:`\"$GitHubRepo`\") { branchProtectionRules(first: 100) { nodes { pattern, id matchingRefs(first: 100) { nodes { name } } } } } }\"}write-host \"Getting policies for repo: $GitHubRepo ...\"$graphql = Invoke-RestMethod -Uri \"https://api.github.com/graphql\" -Method POST -Headers $AuthenticationHeader -ContentType 'application/json' -Body ($body | ConvertTo-Json) # | ConvertTo-Json -Depth 10write-host \"\"foreach($policy in $graphql.data.repository.branchProtectionRules.nodes) { if($policy.pattern -like $PatternToDelete) { write-host \"Deleting branch policy: $($policy.pattern) ...\" $bodyDelete = @{ query = \"mutation { deleteBranchProtectionRule(input:{branchProtectionRuleId: `\"$($policy.id)`\"}) { clientMutationId } }\" } $toDelete = Invoke-RestMethod -Uri \"https://api.github.com/graphql\" -Method POST -Headers $AuthenticationHeader -ContentType 'application/json' -Body ($bodyDelete | ConvertTo-Json) if($toDelete -like \"*errors*\") { write-host \"Error deleting policy: $($policy.pattern)\" -f red } else { write-host \"Policy deleted: $($policy.pattern)\" } }}UsageAs an example, if I wanted to clean up all of the branch protection rules on my feature branches, the script can be called like:./github-delete-branch-protection.ps1 -PatternToDelete \"feature*\" -PersonalAccessToken \"xxx\" -GitHubOrg \"myorg\" -GitHubRepo \"myrepo\"Alternatively, if I wanted to delete ALL branch protection rules, I can use a \"*\" wildcard to delete them all:./github-delete-branch-protection.ps1 -PatternToDelete \"*\" -PersonalAccessToken \"xxx\" -GitHubOrg \"myorg\" -GitHubRepo \"myrepo\"OutputHere’s an example of an output / logs from the script:Getting policies for repo: gh-cli-get-branches-example ...Deleting branch policy: test1 ...Policy deleted: test1Deleting branch policy: test2 ...Policy deleted: test2NotesIf you have more than 100 branch protection rules that you are cleaning, update the branchProtectionRules(first: 100) and matchingRefs(first: 100)" }, { "title": "GitHub: Block Pull Requests if a Vulnerable Dependency is Added", "url": "/posts/dependency-review-action/", "categories": "GitHub, Advanced Security", "tags": "GitHub, GitHub Advanced Security, GitHub Actions, Pull Requests, Policy Enforcement, Branch Protection Rules", "date": "2022-04-06 15:00:00 -0500", "snippet": "OverviewGitHub has added a new Dependency Review action to help keep vulnerable dependencies out of your repository! One of the complaints with the way Dependabot Security Alerts works in GitHub is that it only works from the default branch. As a result, you aren’t alerted that you are adding a vulnerable package until after you have already merged to the default branch. The previous solution to this was the Dependency Review (rich diff) in a pull request, but this was slightly hidden and there was no enforcement capabilities.Note that the new Dependency Review action still requires a GitHub Advanced Security license, as mentioned in the GitHub Changelog blog post: The dependency review action is available for use in public repositories. The action is also available in private repositories owned by organizations that use GitHub Enterprise Cloud and have a license for GitHub Advanced Security.Dependency Review ActionThe action is relatively simple (no inputs as of yet) - and here’s some additional documentation.name: 'Dependency Review'on: [pull_request]jobs: dependency-review: runs-on: ubuntu-latest steps: - name: 'Checkout Repository' uses: actions/checkout@v3 - name: 'Dependency Review' uses: actions/dependency-review-action@v1ResultsTo try this at home, you can attempt to \"tar\": \"2.2.2\" to the dependencies section of your package.json file. This will cause the action to fail since there are several vulnerabilities in this version of tar:Dependency Review Action preventing a pull request with a vulnerable dependency addedI think this is much better than the prior option for finding/preventing vulnerable dependencies in a pull request:The previous option for dependency review in a pull request (rich diff)Making this a required status checkHow ToTo make this a required status check on the pull request, follow these instructions: The first thing you need is a public repository (GHAS is free for public repos) or a private repository with the GitHub Advanced Security license enabled Under the Settings tab in the repository, navigate to Branches Create a branch protection rule for your default branch - check the ‘Require status checks to pass before merging’ box Add the dependency review job as a status check - in the example above, it’s dependency-review If you don’t see the dependency-review to add as a status check to the branch protection, it won’t appear as an option until you initiate at least one PR on the repository that triggers this job. Branch Protection Policy with the dependency-review status check configuredSummaryThis new Dependency Review action uses the dependency review API endpoint to determine if you are adding a new vulnerable package version to your codebase. It doesn’t catch/block if there are any vulnerable dependencies, only dependencies added/modified in the pull request." }, { "title": "How to use gh auth login CLI Programmatically in GitHub Actions", "url": "/posts/gh-auth-login-in-actions/", "categories": "GitHub, Actions", "tags": "GitHub, GitHub Actions, gh cli", "date": "2022-03-28 12:00:00 -0500", "snippet": "OverviewQuick post here since I have to search for this every time I try to use the gh cli in GitHub Actions. In order to use the gh cli, you typically have to run gh auth login to authenticate. If you are running this from an interactive session (ie: on your local machine), you are provided with some prompts to easily authenticate to GitHub.com or GitHub Enterprise Server. If you try to do this from an command in a GitHub Actions, the action will just stall out and you will have to cancel since gh auth login is intended to be done in an interactive session.There is a gh auth login --with-token in the docs that provides an example for reading from a file, but if you’re running in a GitHub Action workflow, your ${{ secrets.GITHUB_TOKEN }} isn’t going to be a file.Example 1 - gh auth loginHere’s an example GitHub Action sample for logging into the gh cli and using gh api to retrieve a repositories topics: steps: - run: | echo ${{ secrets.GITHUB_TOKEN }} | gh auth login --with-token gh api -X GET /repos/${{ GITHUB.REPOSITORY }}/topics --jq='.names'Example 2 - env variableHowever, there is another way. If you try to run a gh command without authenticating, you will see the following error message:gh: To use GitHub CLI in a GitHub Actions workflow, set the GH_TOKEN environment variable. Example: env: GH_TOKEN: ${{ secrets.GITHUB_TOKEN }}If you are using the gh cli in multiple steps or jobs in a workflow, setting the GH_TOKEN as an env might be better:env: GH_TOKEN: ${{ secrets.GITHUB_TOKEN }}jobs: prebuild: runs-on: ubuntu-latest steps: - run: | gh api -X GET /repos/${{ GITHUB.REPOSITORY }}/topics --jq='.names' build: runs-on: ubuntu-latest steps: - run: | gh api -X GET /repos/${{ GITHUB.REPOSITORY }}/branches --jq='.[].name'With this, you will notice you don’t have to run gh auth login at all.You can alternatively use jobs.&lt;job_id&gt;.steps[*].env or jobs.&lt;job_id&gt;.env to set an environment variable for a particular step or job instead of the whole workflow, but this would have to be added to each step/job that you were running gh commands in." }, { "title": "Migrate Azure DevOps Work Items to GitHub Issues", "url": "/posts/migrate-azure-devops-work-items-to-github-issues/", "categories": "GitHub, Migrations", "tags": "GitHub, GitHub Issues, Azure DevOps, Azure Boards, Work Items, Scripts", "date": "2022-03-11 04:00:00 -0600", "snippet": "OverviewQuick post since most of this is in the README in the repo, but I created a Powershell script to migrate Azure DevOps work items to GitHub Issues. It’s certainly not perfect, but there wasn’t anything else out there I could find. If you do find something better, please do let me know!The repo: https://github.com/joshjohanning/ado_workitems_to_github_issuesExampleLink to example of migrated issue in GitHubScreenshot:Example of work item migrated from Azure DevOps to GitHub Issues" }, { "title": "Use Dependabot in GitHub with Azure Artifacts", "url": "/posts/github-dependabot-with-azure-artifacts/", "categories": "GitHub, Dependabot", "tags": "GitHub, Dependabot, Azure Artifacts, Azure DevOps, GitHub Packages, Pull Requests", "date": "2022-03-10 08:00:00 -0600", "snippet": "OverviewIf you have heavy investment in Azure Artifacts, it can be hard to fully transition to GitHub Packages. However, there is a bit of a transition. In GitHub, while you can see a list of packages the organization level, the packages are installed to a specific repository. For further detail, here are the instructions for pushing various package ecosystems to GitHub: npm NuGet Maven DockerAlright but you might be thinking, if I’m not using GitHub Packages, won’t Dependabot not work then? Well, no. Dependabot is not just for keeping your public packages up to date - Dependabot also supports private feeds, including Azure Artifacts!ConfigurationFor this to work, you just have to set up a Dependabot secret. I called my secret AZURE_DEVOPS_PAT below.Here is the full .github/dependabot.yml configuration:version: 2registries: npm-azure-artifacts: type: npm-registry url: https://pkgs.dev.azure.com/jjohanning0798/PartsUnlimited/_packaging/npm-example/npm/registry/ username: jjohanning0798 password: ${{ secrets.AZURE_DEVOPS_PAT }} # Must be an unencoded passwordupdates: - package-ecosystem: \"npm\" directory: \"/\" registries: - npm-azure-artifacts schedule: interval: \"daily\"Confirming it worksShortly after committing the .dependabot.yml file, we can confirm it works as there’s a new PR from Dependabot:Pull request created by DependabotWe can also look at our Dependabot logs:Dependabot logs showing that there is a new package version from Azure ArtifactsTroubleshootingDon’t use token with Azure DevOpsIf you follow the Dependabot documentation for NuGet that’s there today, for example, it has you use a token property instead of username and password:registries: nuget-azure-devops: type: nuget-feed url: https://pkgs.dev.azure.com/.../_packaging/My_Feed/nuget/v3/index.json token: ${{secrets.MY_AZURE_DEVOPS_TOKEN}} # this doesn't workIf you check your Dependabot logs, you will probably see 401 or private_source_authentication_failure errors. This is because Azure Artifacts needs to use basic authentication, which using the username and password fields provide. The username isn’t used, but the password has to be an unencoded personal access token.registries: nuget-azure-devops: type: nuget-feed url: https://pkgs.dev.azure.com/.../_packaging/My_Feed/nuget/v3/index.json username: octocat@example.com password: ${{secrets.MY_AZURE_DEVOPS_TOKEN}} # this worksAlternatively, you could still use token, but just append a : at the end of the PAT as mentioned in this issue here.Pull request limitAnother reason you might not be seeing your pull request from an outdated dependency in Azure Artifacts is if the pull request limit is not defined. By default, the limit is 5, so Dependabot will only create 5 pull requests for version updates as to not inundate you. If you check your pull requests, you might see you have more than 5, but some of those might be Dependabot Security Alerts, which don’t count to that limit.See the docs, but here’s an example (see: open-pull-requests-limit on line 15):version: 2registries: npm-azure-artifacts: type: npm-registry url: https://pkgs.dev.azure.com/jjohanning0798/PartsUnlimited/_packaging/npm-example/npm/registry/ username: jjohanning0798 password: ${{ secrets.AZURE_DEVOPS_PAT }} # Must be an unencoded passwordupdates: - package-ecosystem: \"npm\" directory: \"/\" registries: - npm-azure-artifacts schedule: interval: \"daily\" open-pull-requests-limit: 15Dependabot misconfigurationIf you have any other misconfiguration, such as the registry names not matching, you will be able to see from the Dependabot logs as well. Here’s an example of such an error where the two registry names didn’t match: The property ‘#/updates/0/registries’ includes the “nuget-azure-artifacts” registry which is not defined in the top-level ‘registries’ definitionSee the docs for the configuration syntax and examples.Re-running DependabotEven though you might have the schedule set to “daily”, Dependabot will run again if you push a change to the .github/dependabot.yml. You can also run it manually at any time by navigating to: Insights Dependency Graph Dependabot Click into the last run, e.g.: “last checked 16 hours ago” Check for updatesCheck for Dependabot updates again manuallySummaryBeing able to use Dependabot with Azure Artifacts is a great way to keep your internally-created packages up to date. Teams can be notified automatically that there’s a new version of the package available and after a successful build with passing unit tests, can accept and merge the PR. If a team doesn’t want to use the updated version, they can simply close the PR and it won’t be re-opened until a new version of the package is released. I always prefer to at least be notified of new versions, so I think this is awesome!If the emails become too much, you can always modify your notification settings 😀." }, { "title": "Ignore Files in GitHub CodeQL Analysis", "url": "/posts/github-codeql-ignore-files/", "categories": "GitHub, Advanced Security", "tags": "GitHub, GitHub Advanced Security, CodeQL", "date": "2022-03-08 12:00:00 -0600", "snippet": "OverviewI was recently working with a customer and we flipped on the security-and-quality query suite and received a a lot of results, mostly in our tests. We wanted a way to ignore these files for the purposes of the CodeQL analysis. One could argue that these should be scanned and acted on too, but you have to start somewhere, right? And there are plenty of use cases where might want to ignore a file for the purposes of the CodeQL analysis.There are a few ways to do this, one through a filter in the UI, and another with actions. I will show you both below!Filter out tests in the UIYou might notice that some of the queries have the phrase (Test) before the file name. CodeQL tries to determine which files are tests and which are application code automatically:CodeQL result found in a test fileYou can filter out Test results in the UI by adding the autofilter:true filter in the search bar:Filtering out CodeQL vulnerabilities in test files with autofilterYou cannot use autofilter:false to filter only test results, however.Exclude files using an actionI found the filter-sarif action that did just what the team wanted to do - filter out all **/*test*.js files! The action isn’t published on the marketplace (yet!), but it is a public GitHub repository, therefore we can use it just like we can any other action that is published to the marketplace.The docs reference some cool patterns you can use - such as ignoring all tests, but even inside of those tests, still report sql-injection vulnerabilities. You can find the ID for this by referencing the CodeQL Query Help page, selecting your language, selecting the query, and find the ID reference. Alternatively, you can dig into the CodeQL GitHub repoistory to find the query and reference the @ID value.Here’s an example:name: \"CodeQL\"on: push: branches: [ main ] pull_request: branches: [ main ] schedule: - cron: '43 22 * * 3'jobs: analyze: name: Analyze runs-on: ubuntu-latest strategy: fail-fast: false matrix: language: [ 'javascript' ] steps: - name: Checkout repository uses: actions/checkout@v2 - name: Initialize CodeQL uses: github/codeql-action/init@v1 with: languages: ${{ matrix.language }} - name: Autobuild uses: github/codeql-action/autobuild@v1 - name: Perform CodeQL Analysis uses: github/codeql-action/analyze@v1 with: upload: false # disable the upload here - we will upload in a different action output: sarif-results - name: filter-sarif uses: zbazztian/filter-sarif@master with: # filter out all test files unless they contain a sql-injection vulnerability patterns: | -**/*test*.js +**/*test*.js:js/sql-injection input: sarif-results/${{ matrix.language }}.sarif output: sarif-results/${{ matrix.language }}.sarif - name: Upload SARIF uses: github/codeql-action/upload-sarif@v1 with: sarif_file: sarif-results/${{ matrix.language }}.sarif # optional: for debugging the uploaded sarif - name: Upload loc as a Build Artifact uses: actions/upload-artifact@v2.2.0 with: name: sarif-results path: sarif-results retention-days: 1If you couldn’t tell by the design of the workflow, this isn’t necessarily skipping these files as part of the scan - it simply strips them out from the .sarif that’s being uploaded to GitHub.After running, you will see that the vulnerability in the excluded test file is now marked automatically as closed:CodeQL result marked as closed after we are excluding test filesIf we click into the vulnerability, we also see the history. For example, I was testing this workflow so it opened and closed a few times:CodeQL result history - when it was opened, closed, and reappearedSummaryYou can either filter out non-application code in the UI or using an action to exclude certain files based on a pattern. Happy scanning!" }, { "title": "GitHub Advanced Security Permissions Chart", "url": "/posts/github-advanced-security-permissions-chart/", "categories": "GitHub, Advanced Security", "tags": "GitHub, GitHub Advanced Security, Dependabot", "date": "2022-03-08 12:00:00 -0600", "snippet": "OverviewI have several posts discussing GitHub Advanced Security, but practically a question that I get often is: “Who can access the alerts on each repository?”I hope to solve that with this permissions / access requirements chart!See also: GitHub Advanced Security Feature ComparisonAccess requirements for security featuresThis chart is loosely based on the one from GitHub, with a few additions, modifications, and clarifications. Feature Read, Triage Write Maintain Admin Security Mgr Org Owner Receive Dependabot alerts       X X X Dismiss Dependabot alerts       X X X Designate others to receive security alerts       X X X Create security advisories       X X X Manage access to GHAS features in the repo       X X X Enable the dependency graph       X X X View dependency reviews X X X X X X View code scanning alerts on pull requests X X X X X X Manage code scanning alerts   X X X X X View secret scanning alerts in a repository   X[1] X[1] X X X Manage secret scanning alerts   X[1] X[1] X X X Access to the org’s security overview         X X Access to the enterprise’s security overview         X[2] X[2] Manage GHAS features at org level         X X Designate Security Managers           X Read access to repo(s) X X X X X[3] X Write access to repo(s)   X X X   X Notes: [1] Repository writers and maintainers can only see alert information for their own commits [2] At the enterprise security overview level, you would only see organizations that you are added as an org owner or security manager [3] Security managers get read-only access to every repository in the organization This chart primarily focuses on GitHub Advanced Security in GitHub Enterprise CloudGranting access to security alertsSecurity alerts for a repository are visible to people with admin access to the repository and, when the repository is owned by an organization, organization owners. You can also give additional teams and people access to the alerts.When adding users to be able to view security alerts, there is a bit of text that explains (emphasis mine): Admins, users, and teams in the list below have permission to view and manage code scanning, Dependabot, or secret scanning alerts. These users may be notified when a new vulnerability is found in one of this repository’s dependencies and when a secret or key is checked in. They will also see additional details when viewing Dependabot security updates. Individuals can manage how they receive these alerts in their notification settings.Note: Organization owners and repository administrators can only grant access to view security alerts, such as secret scanning alerts, to people or teams who have write access to the repo.Custom Repository RolesOrganization administrators can create Custom Repository Roles to customize and fine-tune different permission sets that repository administrators can grant. For example, I want to create a role that allows users to have Write access AND be able to view/dismiss Dependabot Alerts:Custom repository roles - creating a custom role to allow viewing/managing of Dependabot alertsChangelog Date Note Mar 11 2021 Adding section about security alerts Mar 08 2021 Initial post " }, { "title": "Docker Container Jobs in GitHub Actions", "url": "/posts/github-container-jobs/", "categories": "GitHub, Actions", "tags": "GitHub, GitHub Actions, Containers", "date": "2022-03-07 13:30:00 -0600", "snippet": "OverviewGitHub Actions has a relatively little known feature where you can run jobs in a container, specifically a Docker container. I liken it to delegating the entire job to the container, so every step that would run in the job will instead run in the container, including the clone/checkout of the repository. This generally works well, but there are some tips and tricks that I can pass along that may be helpful, especially if running in a self-hosted runner scenario.Why would you want to use a container job, you may ask? Well imagine you have a Python application that uses a specific version of Python. Okay, simple enough, we can just use the Setup Python action to install the right version. But what if we also require a specific / non-standard version of Node? And MySQL? We could use a script and install all our prerequisites using apt install, but this takes time. Over dozens of CI jobs, the extra minutes add up and you might even run against the cap of your limit. So instead, we can use a container job that has all the prerequisites our application needs to build / run already pre-installed.I won’t really be covering it, but there is also the option to run a service container alongside your job. This would be useful if running tests against a containerized copy of a database, for example. The documentation uses Redis as an example. Similar with Docker Container Actions.CaveatsUsually, I put the caveats after the implementation, but there are enough important ones here to lead with. If none of these apply, head to the Implementation section. Containers in GitHub Actions, including Container Jobs, Service Containers, and Docker Container Actions only work on Linux runners - they will not run on Windows runners 😔 Some Marketplace actions, such as Checkmarx, are Docker Container Actions, therefore they won’t run on Windows Container jobs/actions can’t run on Windows or MacOS If you are using Docker to run the runner without doing the docker-in-docker magic, you might see an error - but if you are using something like actions-runner-controller, this is a non-issue Error mentioned in this issue Container jobs/actions can’t run within another container unless you have docker-in-docker setup You cannot override the working directory that gets mapped in - the /_work/ directory on the host is mapped to /__w/ in the container This is only a problem if you had intended to use an alternative work directory with permissions already set up in the container We can pass in additional options to use in GitHub for the container job, but Docker our options get added to the end of the original Docker command and subsequent --workdir options are ignored Mentioned in this issue Relevant errors: /usr/bin/git init /__w/container-job-test/container-job-test/__w/container-job-test/container-job-test/.git: Permission deniedError: The process '/usr/bin/git' failed with exit code 1 and Deleting the contents of '/__w/container-job-test/container-job-test'Error: Command failed: rm -rf \"/__w/container-job-test/container-job-test/.git\"rm: cannot remove '/__w/container-job-test/container-job-test/.git': Permission denied A fix was to chmod the /_work/ directory on the host to work around this permissions issue The default shell for run steps inside a container is sh instead of bash. This can be overridden with jobs.&lt;job_id&gt;.defaults.run or jobs.&lt;job_id&gt;.steps[*].shell. This is important because bashisms, such as if statements that contain [[ ]], will not work in a sh script As an example, you might see an [[: not found error when running the container job that works when not running as a container job ImplementationThe full syntax for using container jobs, such as specifying ports, volumes, and options, is available here.In this example, I am building my own Docker image, publishing to the repository, and using the image in a subsequent workflow as a container job, shown below:name: Container Jobon: push: branches: mainenv: test: valuejobs: container: runs-on: ubuntu-latest container: # image: 'ubuntu:20.04' # can also use this to test image: 'ghcr.io/${{ github.repository }}:latest' credentials: username: ${{ github.ref }} password: ${{ secrets.GITHUB_TOKEN }} env: actor: ${{ github.actor }} testjob: here is value steps: - uses: actions/checkout@main - name: run ls run: ls # all these below work - name: print actor env var run: echo \"$actor\" - name: print directly github context run: echo \"${{ github.actor }}\" - name: print repo secret run: echo \"${{ secrets.TEST_SECRET }}\" - name: print job env var run: echo \"$testjob\" - name: print root env var run: echo \"$test\" - name: condition with root env var if: ${{ env.test == 'value' }} run: echo \"$test\" When you run the workflow, you will notice an additional log entry to initialize the container. Subsequently, all steps in the job will run inside of the container:Successful container jobFor those wondering, here is what a sample DOCKERFILE for this looks like - hint there’s nothing special. You can also test this with good ‘ol ubuntu:20.04 (or ubuntu:latest).Perhaps more interestingly, my workflow for publishing my Docker image is here.SummaryI’ve used Container Jobs in Azure DevOps before, and I was excited to see we had similar functionality in GitHub! This can be much more practical than writing a large script to apt install everything for each job run. Just note some of the caveats, most of which only apply to self-hosted and non-Linux runners.You can take this to the next step, instead of running your jobs in containers, you could additionally run your runners in containers using something like actions-runner-controller. actions-runner-controller, which is running a runner in a container in k8s, supports running container jobs with Docker actions no problem!" }, { "title": "ApproveOps: GitHub IssueOps with Approvals", "url": "/posts/github-approveops/", "categories": "GitHub, Actions", "tags": "GitHub, GitHub Issues, GitHub Actions, GitHub Apps, IssueOps, ChatOps", "date": "2022-02-08 08:30:00 -0600", "snippet": "OverviewThis is follow-up to my previous post, Demystifying GitHub Apps: Using GitHub Apps to Replace Service Accounts, where I go over the basics of creating a GitHub App and accessing its installation access token in an actionI was working with a customer and we built out a self-service migration solution to migrate from GitHub Enterprise Server to GitHub Enterprise Cloud. Instead of building our own custom interface, we decided to leverage GitHub Issues as the intake and GitHub Actions as the automation mechanism. This is often referred to as “IssueOps”.There are several great examples of IssueOps on GitHub, as well as my co-worker’s ChatOps workflow.The benefit of using GitHub and IssueOps for something like this is the transparency of the process - the Issue is available for everyone to see as well as the logs of the Action. The customer just inputs the Git repository URL’s and a few other inputs, the issue body is parsed, and a pre-migration comment is automatically posted back to the issue by our bot with a slash command that is used to trigger the migration.However, a requirement for the customer was to have a way to approve the migration. We could have had an approval on an environment on the job, but that interface brought you away from the issue. For example, after someone initiates a deployment, the requestor doesn’t necessarily know it’s just sitting for an approval. While the approver(s) would get an email and optionally a push notification in the GitHub mobile app, it doesn’t seem there is anything that shows up under the GitHub notifications bell.If only there was a way to only allow the deployment if someone authorized issued some sort of command ahead of time in the issue… This is where ApproveOps comes in!The Solution: ApproveOpsAs I hinted, ApproveOps is a simple extension of IssueOps that requires a slash command in the issue comment body from someone in the ‘migration approval’ team. In our solution, the slash command to run the deployment was /run-migration and our approval command we used was /approve.If a user runs the migration command without someone approving ahead of time, a bot will comment on the issue saying that it isn’t yet approved and to have someone in the approval team approve the migration by entering in the approval command. If someone who isn’t in the specified approval team tries to approve the migration, their approval comment will simply be ignored because they aren’t in the approval team in GitHub.Here’s how it looks and works in the issue:ApproveOps sample - the /run-migration command doesn’t run the workflow unless someone authorized has commented /approveAnd here is how one of the runs looks in GitHub Actions:The ApproveOps run in GitHub Actions - the migration job is skipped if no one has approved in the issue yetThe CodeI recently created a GitHub Action published on the marketplace that consolidates the various actions and bash commands. If you’re not interested in using the marketplace action or want to extend what I’ve done, my GitHub Actions workflow sample is still available in its entirety.Here’s how you can use my Action in a GitHub Action workflow:name: Approve Opson: issue_comment: types: [created, edited]jobs: approveops: runs-on: ubuntu-latest if: contains(github.event.comment.body, '/run-migration') # optional - if we want to use the output to determine if we run the migration job or not outputs: approved: ${{ steps.check-approval.outputs.approved }} steps: - name: ApproveOps - ApproveOps in IssueOps uses: joshjohanning/ApproveOps@v1 id: check-approval with: app-id: 170284 app-private-key: ${{ secrets.PRIVATE_KEY }} team-name: approver-team fail-if-approval-not-found: false migration: runs-on: ubuntu-latest needs: approveops # optional - if we want to use the output to determine if we run the migration job or not if: ${{ steps.approveops.outputs.approved == 'true' }} steps: - run: echo \"run migration!\"Setup and ExplanationPrerequisites Since accessing team membership is outside the scope of the GitHub Token, we have to either use our GitHub App created in this related post or create a PAT with read:org scope and use it to get the team membership At least one member in the approver team, otherwise jq won’t be able to find the .[].login property (this could probably be written more defensively :) )ExplanationI am using a bash script in my composite action to: Get a list of users in the approval team Get a list of comments on the issue (note that I am converting the comments to base64 otherwise comments that had spaces in them would throw the loop off - this was a good resource for explaining that) Check that the comment issue body contains the /approve command If so, check if the user who posted the /approve command is in the approval team (from step 1) by using a grep command Setting an output parameter depending if the migration is authorized to run or not If there aren’t any authenticated approvals, I’m also setting a user-friendly error message to be helpful when looking at why the action run failed Afterwards, we use the output parameter and if: logic to post the write comment on the workflow, either requesting proper approval or informing the user that the migration will now run since it has been approved I’m using additional if: logic and the approveops jobs’ output to determine if the migration job should run or not As an alternative, I could see one omitting this using a single job with additional if: logic on the rest of the migration steps, but this could be messy If using the same job and you didn’t mind seeing failed runs in the UI because of lack of approvals, you could fail the workflow run by setting fail-if-approval-not-found: true Wrap-upThere’s definitely room for improvement here, but I think this is a good starting point for you to get with your own ApproveOps / IssueOps workflow.If you do what I’m doing here, by creating your own GitHub App on your organization and use that identity to write your comment, it will be able to properly @team mention in the comment and everything!Report back if you make any enhancements!" }, { "title": "Demystifying GitHub Apps: Using GitHub Apps to Replace Service Accounts", "url": "/posts/github-apps/", "categories": "GitHub, Actions", "tags": "GitHub, GitHub Actions, GitHub Apps, GitHub Issues", "date": "2022-02-07 20:00:00 -0600", "snippet": "OverviewIn GitHub Actions, the GitHub Token works very well and is convenient for automation tasks that require authentication, but its scope is limited. The GitHub Token is only going to allow us to access data within the repository (such as issues, code, packages), but what if you need to authenticate to another repository, or access organizational information such as teams or member lists? GitHub Token is not going to work for that. Your alternatives are: Use someone’s Personal Access Token (PAT) - but what happens if that person leaves? Or if you need to write back to an issue, for example, it’s going to look like it came from that user Create a service account - but this is going to consume a license, and you still have to manage with vaulting and storing a long-lived PAT somewhere, and if that PAT gets exposed, you’re opening yourselves up to a huge security risk GitHub Apps!In this post, I will go through the setup and usage of GitHub Apps in an Actions workflow with two scenarios: Using a GitHub App to grant access to a single repository and Using a GitHub App as a rich comment bot.And don’t worry - you don’t need any programming experience to create a GitHub App!GitHub AppsGitHub Apps are certainly preferred and recommended from GitHub. From GitHub’s documentation, this fits our exact use case: GitHub Apps are the official recommended way to integrate with GitHub because they offer much more granular permissions to access data. GitHub Apps are first-class actors within GitHub. A GitHub App acts on its own behalf, taking actions via the API directly using its own identity, which means you don’t need to maintain a bot or service account as a separate user.GitHub Apps also have a higher API rate limiting threshold than requests from user accounts and GitHub Actions. Installed in an Enterprise, GitHub Apps can have up to 15,000 requests per hour whereas the limit is 5,000 requests per hour coming from a user account and 1,000 requests per hour coming from GitHub Actions.Caveats Each organization can only own up to 100 GitHub Apps You’ll have to be an organization owner to create and install a GitHub app in an organization Each installation access token is only valid for a maximum of one hourCreating a GitHub AppCreating a GitHub App is pretty straightforward! I’ll defer to GitHub’s documentation for the details, but here’s a quick overview: Navigate to the organization’s settings page Expand the “Developer Settings” section at the bottom of the page and navigate to GitHub Apps Click “New GitHub App” in the upper right-hand corner Start filling in the details! The name and Homepage URL doesn’t matter much right now (but it does need a valid URL here) What does matter is the “Webhook URL” - if we want to use this GitHub App in the next section, we’ll need to grab the installation ID. The easiest way to do that is to start a new channel at smee.io and use the URL of the channel as the Webhook URL. Grant it the repository permissions, organization permissions, user permissions, and what events to subscribe to - for the examples in this blog post, we’ll grant read-only access to repository / contents, read &amp; write access to repository / issues, and read-only access to organization / members - if you change this after the it’s already been installed to an organization, you’ll have review and re-approve the permission changes for the GitHub App After creation, you should see a “ping” entry in your smee.io channel - this is a confirmation that the app was created On the left-hand menu, you should now have a few options, one of those being “Install App” - click it, and install the app to the organization in your smee.io channel, you should have a new payload from the installation - expand the “installation” property to find your “installation ID” - this is the ID that you’ll need to use in the next section You’ll also want to grab your App ID here (although note the App ID can also be found within GitHub) Lastly, navigate back to your GitHub App’s administration page and generate a private key for the app - download the file and grab the contents of the certificate by opening it in VSCode or if you are on macOS: cat approveops-bot.2022-02-07.private-key | pbcopyAn example of an Installation ID and App ID from a payload in smee.io🎉 We now have everything we need to use the app in GitHub Actions! 🥳Using the GitHub App in a GitHub Actions workflowThere are a couple different actions to use such as: navikt/github-app-token-generator peter-murray/workflow-application-token-action jnwng/github-app-installation-token-action tibdex/github-app-token (the one I am using below)I like navikt’s, jnwng’s, and tibdex’s versions because it doesn’t require the GitHub App to be installed on the repository that the action is running from whereas peter-murray’s does. That’s fine, but if the App must be installed on every repository, we’re not saving a ton with the app over the PAT (except that the GitHub App’s token has a built-in expiration). navikt’s version is a Docker-based action. I’m typically going to prefer a node-based action if given the preference since typically a Docker action takes a little bit longer to initiate and requires one additional component to be installed if using self-hosted runners. jnwng’s and tibdex’s actions are node-based actions, and both certainly work. I slightly prefer tibdex’s because you can either pass in an installation_id or not, depending on if the app is installed on the repository or not.It’s really quite simple now that you have the installation ID, the app ID, and the private key. The only prerequisite is to create a secret on the repository (or organization) with the private key’s contents. I named my secret PRIVATE_KEY.Scenario 1: Using a GitHub App to grant access to a single repositoryA customer had a repository that nearly every Actions workflow was going to need to access at deploy-time. For the proof of concept, one of the admins on the team created a PAT and added it as an organizational secret. The problem is though that if the PAT is compromised, that PAT has access to all the repositories in the organization.If there’s a centralized repository that every team needs to access, you can use the GitHub App to grant access to that repository and that repository alone. Note that you could also use deploy keys for this, but that requires you to use the SSH protocol when cloning. We’ll continue as if the GitHub App is the preferred way to go so that you can understand the process.Here’s the action code to generate and sign an installation access token for authenticating with GitHub: steps: - uses: tibdex/github-app-token@v1 id: get_installation_token with: app_id: 170544 installation_id: 23052920 private_key: ${{ secrets.PRIVATE_KEY }} # clone using the `actions/checkout` step - name: Checkout uses: actions/checkout@v2.4.0 with: repository: my-org/my-repo token: ${{ steps.get_installation_token.outputs.token }} path: my-repo # run the git clone ourselves name: Clone Repository run: | mkdir my-repo-2 &amp;&amp; cd my-repo-2 git clone https://x-access-token:${{ steps.get_installation_token.outputs.token }}@github.com/my-org/my-repo.gitWith the GitHub app installed on the my-org/my-repo-2 repository and passing in the installation ID to the action, we have access to clone the repository even though it’s not the repository that the workflow is running from. We can also use the token generated here as a Bearer token for GitHub API requests, assuming it has the access.Successful Git clone using a GitHup App🎉 Repo cloned! 🥳Scenario 2: Using a GitHub App as a rich comment botI’ll often use the peter-evans/create-or-update-comment action to create a comment on a pull request or issue. Typically, I’ll just use the $ for the token which comments as the github-actions bot, and it looks great!GitHub Actions Comment Bot using GitHub Token from the Action runHowever, if you look closely, you might notice something: since the GitHub Token only has access to the repository, it can’t create the proper @team mention in the comment. There is no hyperlink there. This might not be super important, but in my case the team was going to use their GitHub notifications to check if there were any issues that needed their attention, so this wasn’t going to work.If we use a PAT and a secret on the repository, we get the @team mention, but it looks like it came from the user who created the PAT:Issues comment from GitHub Actions using a PATInstead, we can use a GitHub App that with read-only permissions on Organization / Members and read &amp; write on Repository / Issues to create the comment and then we’ll have the mention as well as not coming from a regular user:Issues comment from GitHub Actions using a GitHub AppHere’s the relevant action code: steps: - uses: tibdex/github-app-token@v1 id: get_installation_token with: app_id: 170544 private_key: ${{ secrets.PRIVATE_KEY }} - if: ${{ steps.check-approval.outputs.approved == 'false' }} name: Create completed comment uses: peter-evans/create-or-update-comment@v1 with: token: ${{ steps.get_installation_token.outputs.token }} issue-number: ${{ github.event.issue.number }} body: | Hey, @${{ github.event.comment.user.login }}! :cry: No one approved your run yet! Have someone from the @joshjohanning-org/approver-team run `/approve` and then try your command again :no_entry_sign: :no_entry: Marking the workflow run as failedYou’ll notice that we didn’t have to pass in an installation ID this time to the action. This is because the GitHub App is installed on the repository, and the action can therefore lookup the installation ID dynamically to get the token.🎉 Issue comment with team mentioning success! 🥳SummaryWhen I first learned about GitHub Apps, I was like, “This is cool, but I’m not going to be writing an app and creating code just for authentication, that’s too much work, I’ll just use a PAT.” However, as you just saw, we created a GitHub App and used it for authentication without tying it to any code.In both scenarios, we use the tibdex/github-app-token action and the installation access token that is an output parameter: ${{ steps.get_installation_token.outputs.token }}. We use this token to make authenticated requests to the API or as the password in Git clones. Alternatively, GitHub has sample Ruby code for creating a and signing the JWT and retrieving an installation ID, but the action is so much simpler!Check out my next post, ApproveOps: Approvals in IssueOps, for more information on the action workflow I’m using in the second scenario.Happy App-ing!" }, { "title": "Miscellaneous GitHub API/GraphQL/CLI Automation Scripts", "url": "/posts/github-misc-scripts/", "categories": "GitHub", "tags": "GitHub, Scripts, gh cli", "date": "2022-01-20 13:00:00 -0600", "snippet": "OverviewI have a large Postman workspace for all my API calls, but it’s sometimes hard to share an example of an API or script with someone. Thus, I decided to create a repo that consolidates my random GitHub scripts into one central spot. Now, I can simply send a link!Here’s the repo: joshjohanning/github-misc-scriptsLayoutI have them categorized by type: api gh-cli graphql scriptsI have readme’s in each of the folders with a brief description of the enclosed scripts.Script ExamplesHere’s an example of some of the scripts I have populated in there so far: download file from github packages (api) - (and my blog post!) create repo (api) download file from private repo (api) download workflow artifacts (api) list enterprise (graphql) create organization (graphql) delete repository branch policy (graphql) get issue id (graphql) get repository branch policies (graphql) get repository id (graphql) transfer issue (graphql) download specific version from github packages (graphql) - (and my blog post!) download latest version from github packages (graphql) - (and my blog post!) sso credential authorizations (gh-cli)OverviewLet me know if you have found any of these useful and/or have improved them!" }, { "title": "Programmatically Download a Package Binary from GitHub Packages", "url": "/posts/github-download-from-github-packages/", "categories": "GitHub, Packages", "tags": "GitHub, GitHub Packages, Maven, NuGet", "date": "2022-01-07 14:30:00 -0600", "snippet": "OverviewWe had a team that wanted to push to GitHub packages, which is relatively easily enough to do and is well documented. However, they had a subsequent job that was building a Docker image where that dependency (the .jar or .war file) was needed.There are a couple of different ways you could think about this.1) Maybe the docker build step should occur in the same job as the mvn build step so that it has access to the same binary outputs1) Perhaps instead of GitHub Packages we create a Release on the repository - we can use an Action to do this and an API to download the release1) If we really just want to download the package binary from GitHub Packages…that should be simple enough, right?Just use the Packages API, right?The API for GitHUb Packages says: With the GitHub Packages API, you can manage packages for your GitHub repositories and organizations.Keyword: manage, such as listing or deleting packages. It doesn’t really imply downloading or retrieiving package assets like the Release API has.Okay, but can’t we just go to the package in the UI and copy the download link?Nope – check the URL of one of the files in my repo: https://github-registry-files.githubusercontent.com/445574648/92585100-6fe8-11ec-8a00-38630c14852f?X-Amz-Algorithm=AWS4-HMAC-SHA256&amp;X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20220107%2Fus-east-1%2Fs3%2Faws4_request&amp;X-Amz-Date=20220107T193611Z&amp;X-Amz-Expires=300&amp;X-Amz-Signature=96f4809aebb229ea01b80832c12e546810837194203927c39a31f2c875b177fd&amp;X-Amz-SignedHeaders=host&amp;actor_id=0&amp;key_id=0&amp;repo_id=445574648&amp;response-content-disposition=filename%3Dherokupoc-1.0.0-202201071835.jar&amp;response-content-type=application%2Foctet-streamPretty nasty huh?After spending a few hours on this, there were a few ways I found to do this with various levels of monstrocities committed in finding. I’ll start with the best / easiest and work my way down.Mysteriously hidden CURL urlIn hindsight, it’s so simple, yet it’s not documented anywhere! I was trying to use the mvn dependency:get/copy cli and kept getting stuck on a ‘401 unauthorized error message. In the logs, I saw the URL to the .jar` file I was trying to download and decided to paste that into my browser. I received a username/password basic auth prompt, and I simply pasted in my PAT as a password and I was able to download that file.Extrapulating to curl, this was how to replicate this in the command line:curl 'https://maven.pkg.github.com/&lt;org&gt;/&lt;repo&gt;/com/&lt;group&gt;/&lt;artifact&gt;/&lt;version&gt;/&lt;file-name&gt;.jar' \\ -H \"Authorization: Bearer ${{ secrets.GITHUB_TOKEN }}\" \\ -L \\ -OAnd because my biggest pet peave is when someone has this awesome blog post but then hides/obfuscates all the good stuff, here’s my actual CURL command I used to download a file:curl 'https://maven.pkg.github.com/joshjohanning-org/sherlock-heroku-poc-mvn-package/com/sherlock/herokupoc/1.0.0-202201071559/herokupoc-1.0.0-202201071559.jar' \\ -H \"Authorization: Bearer ${{ secrets.GITHUB_TOKEN }}\" \\ -L \\ -OThe -L is important here as this tells curl to follow redirects. Without it, you’ll get a ‘301 Moved Permanently’ because it’s trying to use use the expanded URL as mentioned above. If you added the -v option to the command, you would see a similar long URL that our curl follows the redirect to.The -O downloads the file locally with the same name as in the URL.GraphQL EndpointYou can also query against the GraphQL Endpoint. It’s not pretty, but here are my examples:Getting the latest download url:{ repository(owner: \"joshjohanning-org\", name: \"sherlock-heroku-poc-mvn-package\") { packages(first: 10, packageType: MAVEN, names: \"com.sherlock.herokupoc\") { edges { node { id name packageType versions(first: 100) { nodes { id version files(first: 10) { nodes { name url } } } } } } } }}This is an example output of that query:{ \"data\": { \"repository\": { \"packages\": { \"edges\": [ { \"node\": { \"id\": \"P_kwDOGo5-nc4AEg_Z\", \"name\": \"Wolfringo.Commands\", \"packageType\": \"NUGET\", \"versions\": { \"nodes\": [ { \"id\": \"PV_lADOGo5-nc4AEg_ZzgDrF3k\", \"version\": \"1.1.2\", \"files\": { \"nodes\": [ { \"name\": \"package.nupkg\", \"url\": \"https://github-registry-files.githubusercontent.com/445546141/cb7fc980-6fc6-11ec-9aa4-02a12c9562b7?X-Amz-Algorithm=AWS4-HMAC-SHA256&amp;X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20220107%2Fus-east-1%2Fs3%2Faws4_request&amp;X-Amz-Date=20220107T195615Z&amp;X-Amz-Expires=300&amp;X-Amz-Signature=5a15e3e70dc86e0485eb8b718374f142d6bdc477c5a99a27ddc7517121ca2818&amp;X-Amz-SignedHeaders=host&amp;actor_id=19912012&amp;key_id=0&amp;repo_id=445546141&amp;response-content-disposition=filename%3Dpackage.nupkg&amp;response-content-type=application%2Foctet-stream\" } ] } } ] } } } ] } } }}To retreive the URL you can curl, the jq is something like:jq -r '.data.repository.packages.edges[].node.versions.nodes[].files.nodes[].url'This is how you can run this GraphQL command via curl and jq to output the download url (generated from Postman):curl 'https://api.github.com/graphql' \\ -s \\ -X POST \\ -H 'content-type: application/json' \\ -H \"Authorization: Bearer xxx\" \\ --data '{\"query\":\"{\\n repository(owner: \\\"joshjohanning-org\\\", name: \\\"Wolfringo-github-packages\\\") {\\n packages(first: 10, packageType: NUGET, names: \\\"Wolfringo.Commands\\\") {\\n edges {\\n node {\\n id\\n name\\n packageType\\n versions(first: 100) {\\n nodes {\\n id\\n version\\n files(first: 10) {\\n nodes {\\n name\\n url\\n }\\n }\\n }\\n }\\n }\\n }\\n }\\n }\\n}\",\"variables\":{}}' \\ | jq -r '.data.repository.packages.edges[].node.versions.nodes[].files.nodes[].url'If you need to grab a specific version, you can use the following GraphQL query:{ repository(owner: \"joshjohanning\", name: \"Wolfringo-github-packages\") { packages(first: 10, packageType: NUGET, names: \"Wolfringo.Commands\") { edges { node { id name packageType version(version: \"1.1.2\") { id version files(first: 10) { nodes { name updatedAt size url } } } } } } }}mvn install and mvI was able to get something like this to work - see my GitHub Action job below: download-job: runs-on: ubuntu-latest steps: - uses: actions/checkout@v2 - name: maven-settings-xml-action uses: whelk-io/maven-settings-xml-action@v20 with: repositories: '[{ \"id\": \"fix_world\", \"url\": \"https://maven.pkg.github.com/joshjohanning-org/sherlock-heroku-poc-mvn-package\" }]' servers: '[{ \"id\": \"fix_world\", \"username\": \"joshjohanning\", \"password\": \"${{ secrets.GITHUB_TOKEN }}\" }]' - name: View settings.xml run: cat ~/.m2/settings.xml - name: Install with Maven run: mvn install -s ~/.m2/settings.xml # wildcard find and mv command to current directory - name: mv run: find /home/runner/.m2 -name \"*herokupoc-1.*.jar\" -exec mv {} . \\;mvn cli - sort ofThis is what we were originally trying to do, so thought I would throw it in here to spur other ideas for other languages. We were trying to use mvn dependenct:get to download the .jar file, but were ultimately uncessful for one reason or another.This is the job we ended with: mvncli: runs-on: ubuntu-latest steps: - uses: actions/checkout@v2 - name: maven-settings-xml-action uses: whelk-io/maven-settings-xml-action@v20 with: repositories: '[{ \"id\": \"fix_world\", \"url\": \"https://maven.pkg.github.com/joshjohanning-org/sherlock-heroku-poc-mvn-package\" }]' servers: '[{ \"id\": \"fix_world\", \"username\": \"joshjohanning\", \"password\": \"${{ secrets.GITHUB_TOKEN }}\" }]' - name: download run: | mvn dependency:get \\ -DgroupId=com.sherlock \\ -DartifactId=herokupoc \\ -Dversion=1.0.0-202201071835 \\ -Dpackaging=jar \\ -Dclassifier=sources \\ -DremoteRepositories=central::default::https://repo.maven.apache.org/maven2,fix_world::::https://maven.pkg.github.com/joshjohanning-org/sherlock-heroku-poc-mvn-packageBut gives us an authentication error: Error: Failed to execute goal org.apache.maven.plugins:maven-dependency-plugin:3.1.1:get (default-cli) on project herokupoc: Couldn’t download artifact: org.eclipse.aether.resolution.DependencyResolutionException: Could not transfer artifact com.sherlock:herokupoc:jar:sources:1.0.0-202201071835 from/to fix_world (https://maven.pkg.github.com/joshjohanning-org/sherlock-heroku-poc-mvn-package): authentication failed for https://maven.pkg.github.com/joshjohanning-org/sherlock-heroku-poc-mvn-package/com/sherlock/herokupoc/1.0.0-202201071835/herokupoc-1.0.0-202201071835-sources.jar, status: 401 Unauthorized -&gt; [Help 1]But this was still useful though as this what led me to just try to curl that .jar file URL successfully :).Wrap-upHopefully this either helped you download a file from GitHub Packages, gives you ideas for other languages, or maybe my struggles convince you to just use a Release in GitHub.I should mention that for some of these you might need to tweak your GITHUB_TOKEN settings to grant it more permissions.Let me know what I’ve missed or if you have any other ideas!" }, { "title": "Migrate SVN to Git", "url": "/posts/migrate-svn-to-git/", "categories": "GitHub, Migrations", "tags": "GitHub, Azure DevOps, SVN", "date": "2022-01-04 23:00:00 -0600", "snippet": "OverviewLet’s face it: Subversion had its time in the sun, but Git is the more modern source control system. If you want to use GitHub and take advantage of all the collaboration and security features, you’re going to want your source code in GitHub. In this post, I describe several options on how to make the jump to Git and GitHub and bring your code (including history!) with you.GitHub ImporterProbably the easiest (and yet the least likely you’ll be able to use) is the GitHub Repo Importer (you can use this for SVN, Mercurial, TFVC, and of course, Git). When you create a new repository in GitHub, there is a little blue link that allows you to Import a repository. If you forget to click the link to import a repository at the time you are creating and naming your GitHub repo, you can still import after repo creation if you haven’t initialized the repository with a Readme or .gitignore.The reason why I say least likely to be able to use is that this requires your SVN server to be publicly accessible from GitHub.com. Most Subversion servers I run into our hosted on-premises, which means you’re pretty much out of luck.If this does work for you, provide the repository url, credentials, and if applicable, which project you are importing, and away you go.Note: According to the documentation, the GitHub Repository Importer is not a feature in GitHub Enterprise Server yet.git-svnThis is the tool I have the most experience with. Using git svn commands, you can create a Git repo from a repo hosted in Subversion (history included). The larger the repo is and the more history there is, the longer the migration will take. Once the repo has been migrated, it can be pushed to GitHub, Azure DevOps, or any other Git host.See the official documentation for migrating from SVN to Git with the git svn commands.The high-level process is as follows: Extract the authors from the SVN repo to create an authors.txt mapping file Modify the mapping file with the author names and email addresses Run git svn clone command Clean up tags and branches Create a Git repo in GitHub / Azure Repos Add the Git repo remote to the local repo and pushSystem Pre-Reqs Windows: Git for Windows TortoiseSVN - When installing, check the box to install the ‘command line client tools’ (not checked by default). Modify or uninstall/re-install if you did not do this with your initial installation. This allows you to run the svn commands from the command line macOS Catalina, Big Sur, Monterey, and greater: Run this command to install the git, svn, and git svn commands:xcode-select --install git should already be installed, so alternatively you can just install svn with the corresponding brew formulae: brew install subversion You can also ensure you have the latest version of git: brew install git or brew upgrade git Option 1: Tags as BranchesThese commands clone an SVN repository to Git, perform some cleanup, and push it to your Git host of choice. Branches will appear as /origin/&lt;branch-name&gt;. In GitHub/Azure DevOps, you can clean this up by re-creating the branch at the root, e.g., creating a new branch /&lt;branch-name&gt; based on /origin/&lt;branch-name&gt;. You can confirm the commit hashes are the same and then delete the branch under /origin. You can delete /origin/trunk without re-creating it because trunk should have been re-created as master.Tags will appear as branches, e.g.: /origin/tags/&lt;tag-name&gt;. You can clean this up by re-creating the tag branch at the root, e.g. /tags/&lt;tag-name&gt; or /&lt;tag-name&gt;. Otherwise, you can manually create a tag in the tags page in GitHub/Azure DevOps based off of the /origin/tags/&lt;tag-name&gt; branch reference. Branches and tags are just pointers in Git anyway, so whether it appears as a tag or a branch, the referenced commit SHA will be the same.Note: In GitHub, when you create a release, you must specify a tag. So, creating a release in the web interface will create a tag. Otherwise, you can use the command line to create tags. Get a list of the committers in an SVN repo: svn log -q http://svn.mysvnserver.com/svn/MyRepo | awk -F '|' '/^r/ {sub(\"^ \", \"\", $2); sub(\" $\", \"\", $2); print $2\" = \"$2\" &lt;\"$2\"&gt;\"}' | sort -u &gt; authors-transform.txt Modify each line to map the SVN username to the Git username, e.g.: josh = Josh &lt;josh@example.com&gt; Make sure the file is encoded as UTF-8 Clone an SVN repo to Git: git svn clone http://svn.mysvnserver.com/svn/MyRepo --authors-file=authors-transform.txt --trunk=trunk --branches=branches/* --tags=tags MyRepo Note: In case of a non-standard layout, replace trunk, branches, and tags with appropriate names Git Tags cleanup (creating local tags off of the remotes/tags/&lt;tag-name&gt; reference so that we can push them): git for-each-ref refs/remotes/tags | cut -d / -f 4- | grep -v @ | while read tagname; do git tag \"$tagname\" \"tags/$tagname\"; git branch -r -d \"tags/$tagname\"; done Git Branches cleanup (creating local branches off of the remotes/&lt;branch-name&gt; reference so that we can push them): git for-each-ref refs/remotes | cut -d / -f 3- | grep -v @ | while read branchname; do git branch \"$branchname\" \"refs/remotes/$branchname\"; git branch -r -d \"$branchname\"; done Add the remote: git remote add origin https://github.com/&lt;user-or-org&gt;/&lt;repo-name&gt;.git Push the local repo to Git host: git push -u origin --all This is what you can expect tags to look like in GitHub after running the migration (as branches):How tags appear in GitHub (as branches) - You can even see that Dependabot created a few branches!And in Azure DevOps:How tags appear in Azure DevOps (as branches)Option 2: Tags as TagsWhen following the above instructions, tags will appear as a branch /origin/tags/&lt;tag-name&gt;. This is usually fine since branches and tags are just pointers in Git anyway, so whether it appears as a tag or a branch, the referenced commit SHA will be the same.If you want to see the tags show under the tags page instead of the branches page in GitHub/Azure DevOps, you can manually create a new tag based on the branch in /origin/tags/, or follow the alternative commands below (particularly step #4).Note: In GitHub, when you create a release, you must specify a tag. So, creating a release in the web interface will create a tag. Otherwise, you can use the command line to create tags. Get a list of the committers in an SVN repo: svn log -q http://svn.mysvnserver.com/svn/MyRepo | awk -F '|' '/^r/ {sub(\"^ \", \"\", $2); sub(\" $\", \"\", $2); print $2\" = \"$2\" &lt;\"$2\"&gt;\"}' | sort -u &gt; authors-transform.txt Modify each line to map the SVN username to the Git username, e.g.: josh = Josh &lt;josh@example.com&gt; Make sure the file is encoded as UTF-8 Clone an SVN repo to Git: git svn clone http://svn.mysvnserver.com/svn/MyRepo --authors-file=authors-transform.txt --trunk=trunk --branches=branches/* --tags=tags MyRepo Note: In case of a non-standard layout, replace trunk, branches, and tags with appropriate names Create Git Tags based on the message that was originally in SVN. git for-each-ref --format=\"%(refname:short) %(objectname)\" refs/remotes/origin/tags \\ | while read BRANCH REF do TAG_NAME=${BRANCH#*/} BODY=\"$(git log -1 --format=format:%B $REF)\" echo \"ref=$REF parent=$(git rev-parse $REF^) tagname=$TAG_NAME body=$BODY\" &gt;&amp;2 git tag -a -m \"$BODY\" $TAG_NAME $REF^ &amp;&amp;\\ git branch -r -d $BRANCH done Git Branches cleanup (creating local branches off of the remotes/&lt;branch-name&gt; reference so that we can push them): git for-each-ref refs/remotes | cut -d / -f 3- | grep -v @ | while read branchname; do git branch \"$branchname\" \"refs/remotes/$branchname\"; git branch -r -d \"$branchname\"; done Add the remote: git remote add origin https://github.com/&lt;user-or-org&gt;/&lt;repo-name&gt;.git Push the local repo to Git host: git push -u origin –all Push the tags to Git host: git push --tags This is what you can expect tags to look like in GitHub after running the migration (as tags):How tags appear in GitHub (as tags)And in Azure DevOps:How tags appear in Azure DevOps (as tags)Clone partial history from SVNThis can be useful if you only want/need history from the last X months or last N revisions cloned from the SVN repository. This can help to speed up the conversion as well as potentially bypassing any errors (such as server timeout). You must pick/find what revision you want to start with manually, though. In this example I am getting everything from revision 3000 to current (HEAD):git svn clone -r3000:HEAD http://svn.mysvnserver.com/svn/MyRepo --authors-file=authors-transform.txt --trunk=trunk --branches=branches/* --tags=tags MyRepoYou can use an SVN client (TortoiseSVN on Windows, SmartSVN on Mac) or git svn log to help you with finding out what revision to start with. Alternatively, if you want to precisely find the previous N revision, you can use the 3rd party scripts found here.MetadataThe --no-metadata option can be used in the git svn command (steps #3 above) for one-shot imports, like we are essentially what we are doing here, but it won’t include the git-svn-id (url) in the new git commit message. If this is a one-shot import, and you don’t want to be cluttered with the old git-svn-id (url), include this option.From the git-svn documentation: Set the noMetadata option in the [svn-remote] config. This option is not recommended. This gets rid of the git-svn-id: lines at the end of every commit. This option can only be used for one-shot imports as git svn will not be able to fetch again without metadata. Additionally, if you lose your $GIT_DIR/svn/**/.rev_map.* files, git svn will not be able to rebuild them.You can compare the difference between adding --no-metadata and not in the examples of my migration runs: Tags as Branches (with --no-metadata) Tags as Tags (without --no-metadata)Note that my initial commit in SVN didn’t have a commit message, that’s why it’s showing “No commit message” for most of the files. git svn migrates commit messages with or without --no-metadata.Resources / BookmarksThis is my stash of references I used that may be helpful for you: Converting a Subversion repository to Git and cleaning up binaries in the process tortoise svn giving me “Redirect cycle detected for URL ‘domain/svn’” Why do I get “svn: E120106: ra_serf: The server sent a truncated HTTP response body” error? How to import svn branches and tags into git-svn? and convert git-svn tag branches to real tags What is the format of an authors file for git svn, specifically for special characters like backslash or underscore? Git svn clone with author name like “/CN=myname” Author not defined when importing SVN repository into Git (make sure the file is encoded as UTF-8) git svn –ignore-paths regex, and How is ignore-paths evaluated in git svn? SVN and KeepAlive (svn: E175002: Connection reset) How to git-svn clone the last n revisions from a Subversion repository? and Git Svn clone certain revision, and continue cloning other revisions in the futuresvn2gitGitHub’s importing source code to GitHub documentation mentions another tool you can use as well - svn2git. I do not have any experience with this tool but wanted to call it out here as another option.Tip MigrationI’d be remiss if I did not mention that there’s always the option of just migrating the tip - meaning, grab the latest code from SVN and start fresh with a new repo in GitHub. Leave all of the history in SVN and start fresh in GitHub by coping in the files, creating a gitignore to exclude any binaries and other unwanted files, and pushing. Ideally, you could keep the SVN server around for a while or make an archive somewhere that it would still be possible to view / recover the history.Understandably, this won’t work for everyone, but it is always an option if the migration options aren’t worth the effort, and you really just care about your most recent code being in GitHub.Wrap-upNow that you have your code migrated to Git, the hard part of moving to GitHub is behind you. Even if you’re not using GitHub, migrating from SVN to Git certainly has its advantages.I will note that once the code is in GitHub, it is technically possible to use svn clients to connect to repositories on GitHub, if you’re in GitHub I think it is wise to use Git like everyone else in GitHub :).Did I miss anything, or have you any improvements to be made? Let me know in the comments!" }, { "title": "Connecting Azure Boards Github App to Multiple Azure DevOps Orgs", "url": "/posts/github-connecting-to-azure-boards-multiple-orgs/", "categories": "GitHub, Integrations", "tags": "GitHub, Azure Boards, Azure DevOps, Work Items", "date": "2021-12-09 22:00:00 -0600", "snippet": "OverviewWe all probably know by now that there is some pretty solid first-party support for linking GitHub to Azure DevOps, specifically, Azure Boards, with the Azure Boards GitHub app. Assuming you have the right permissions, the setup is straight forward. When going through and setting up the GitHub app, you’ll pick the Azure DevOps organization and project that you want to link to.And this works great - however, what isn’t as clear, what if you have a GitHub organization that you want to link to multiple Azure DevOps Projects or Organizations? Going through the Azure Boards installation process only allows you to select a single Azure DevOps Organization and linking to a single Project. Unless your team is following the One Project to Rule Them All strategy, you start to realize that this might not be a very tenable solution. The documentation seems to indicate that connecting our GitHub organization to more than one Azure DevOps organization is not recommended (nor possible).ConfigurationAfter a little playing around, here are the steps that I followed in order to satisfy our scenario of linking our GitHub organization to multiple Azure DevOps organizations with the Azure Boards GitHub app: Install the Azure Boards app to your GitHub org - select the Azure DevOps Org #1 organization that you want to link to as well as what GitHub repo(s) to link to Navigate to the GitHub organization –&gt; Settings –&gt; Installed GitHub Apps –&gt; Azure Boards and make a change (ie select a new repo) and click ‘Save’ If you had selected ‘All repositories’ in step #1, you will have to select ‘Only select repositories’ instead and select the repos by hand to be able to click ‘Save’ on this page Link it to Azure DevOps Org #2 For each of the Azure DevOps organizations, navigate to the project –&gt; project settings –&gt; GitHub Connections to verify the repo mappings are correct - add/remove GitHub repositories if necessaryNow you have a single GitHub organization linked to multiple Azure DevOps organizations!ExampleHere it is in action - I created a commit in each repository in GitHub. They both happen to link to AB#1 since these are both new Azure DevOps organizations and this was the first work item I created in each. I also wanted to prove that there wouldn’t be any conflicts of the links that are created.Azure DevOps Org #1 linked to GitHub repo AAzure DevOps Org #2 linked to GitHub repo BGotchas Note that 1 GitHub repo would only be linked to 1 AzDO org/project at a time though - if you try to link a GitHub repo to more than 1 AzDO org, you get a fun null error message: null error message trying to add an already-linked GitHub repository If you don’t see your GitHub org in the Azure DevOps project settings –&gt; GitHub Connections when adding/removing GitHub repositories, try launching an incognito window to re-force the GitHub authentication flow to be able to authenticate to another GitHub repo We once saw that the GitHub connection was disabled, but we think that was after trying to create a new GitHub connection from a new Azure DevOps org directly in Azure DevOps without first going through GitHub - if this happens, you should be able to manually re-enable the GitHub connectionSummaryGitHub works best when using a single org model. If you wanted to use the Azure Boards integration to link to multiple Azure DevOps organizations, you might be displeased at first after reading the documentation - but hopefully the steps in this article will help you configure the integration properly!" }, { "title": "GitHub Advanced Security Feature Comparison", "url": "/posts/github-advanced-security-feature-chart/", "categories": "GitHub, Advanced Security", "tags": "GitHub, GitHub Advanced Security, Dependabot", "date": "2021-12-03 16:30:00 -0600", "snippet": "OverviewGitHub Advanced Security (GHAS) is an addon for those on GitHub Enterprise. While it costs extra, the code scanning, secret scanning, and the dependency review feature set is quite impressive. Pretty much all of these features are enabled by default for Public Repos hosted on github.com (with the exception of the organization-level security overview and custom secret scanning patterns), so you can easily create a repo with some sample code from your personal GitHub account to test.Follow updates in the Changelog blog for the latest updates on GitHub Advanced Security!GitHub Advanced Security Feature ComparisonI made this chart a while back for a client when helping them determine if the GHAS addon was worth it to them: Feature GHE GHE + GHAS Public Repos Dependency Graph X X X Dependabot Alerts for Vulnerable Dependencies X X X Dependabot Security Updates (PRs for vulnerabilities) X X X Dependabot Version Updates (PRs for package updates) X X X GitHub Security Advisories X X X Security Policies X X X Security Overview for the Org (Beta)   X n/a Security Overview for the Enterprise (Beta)   X n/a CodeQL Code Scanning   X X Dependency Review in Pull Request (rich diff)   X X Dependency Review Action (Beta)   X X Secret Scanning   X X * Secret Scanning - Custom Patterns   X   Secret Scanning - Push Protections (Beta)   X   Notes: GHE = GitHub Enterprise GHAS = GitHub Advanced Security * - Note that you won’t see a secret scanning menu for public repos, you will just get an email when a secret was committed to the repo and that the secret was (likely) automatically rolled or disabled If you subscribe to GitHub Advanced Security and have a public repo, you can still see the alerts This chart primarily focuses on GitHub Enterprise Cloud, but note that Advanced Security is available for GitHub Enterprise Server 3.0 or higherAbout DependabotThere are a few components of Dependabot, and while I tried to list each feature individually in the chart, I wanted to call out a helpful quote of the documentation to help describe part of the differences between version updates and security updates: About Dependabot version updates: When Dependabot identifies an outdated dependency, it raises a pull request to update the manifest to the latest version of the dependency. For vendored dependencies, Dependabot raises a pull request to replace the outdated dependency with the new version directly. You check that your tests pass, review the changelog and release notes included in the pull request summary, and then merge it. For more information, see “Enabling and disabling Dependabot version updates.” If you enable security updates, Dependabot also raises pull requests to update vulnerable dependencies. For more information, see “About Dependabot security updates.” When Dependabot raises pull requests, these pull requests could be for security or version updates: Dependabot security updates are automated pull requests that help you update dependencies with known vulnerabilities. Dependabot version updates are automated pull requests that keep your dependencies updated, even when they don’t have any vulnerabilities. To check the status of version updates, navigate to the Insights tab of your repository, then Dependency Graph, and Dependabot. Dependabot version updates requires creating a dependabot.yml configuration file in your repository whereas Dependabot security updates automatically locates supported package manifest files and alerts you when it contains vulnerable dependencies.Dependabot version updates supported package ecosystems differs from that of Dependabot security updates.Changelog Date Note Apr 06 2022 Adding Dependency Review Action (Beta) Apr 04 2022 Adding Secret Scanning - Push Protections (Beta) Mar 07 2022 Adding new Security Overview for the Enterprise (Beta) and secret scanning note for public repos Jan 26 2022 Adding Dependabot section; reorganized chart Dec 03 2021 Initial post " }, { "title": "Powerlevel10k Zsh Theme in GitHub Codespaces", "url": "/posts/github-codespaces-powerlevel10k/", "categories": "GitHub, Codespaces", "tags": "GitHub, GitHub Codespaces, VS Code", "date": "2021-11-23 16:00:00 -0600", "snippet": "OverviewHello 👋 ! This is my first post since joining the GitHub FastTrack team last week. I’m still learning a lot of information as well as tips and tricks from other Hubbers. One of the things I have started playing around more with now is GitHub Codespaces. I wanted to have my GitHub Codespace to have the exact same look and feel that my local environment had - including my Zsh plugins and Zsh theme: Powerlevel10k. I found a post from Burke Holland that got me close, but it didn’t have the Powerlevel10k bit in it. If you are interested to seeing my local development environment setup, see: My macOS Development Environment: iTerm2, oh-my-zsh, and VS CodeWhat is GitHub Codespaces?I’ll try not to belabor the point, but GitHub Codespaces is a convenient way for teams to build a consistent development environment baseline that everyone can tap into. Gone are the days where the amount of time spent setting up a new development environment when switching teams or receiving a new laptop is measured in DAYS. I could use a machine (or iPad!) anywhere in the world, and if I connected to my Codespace, I could start development immediately.By default, Codespaces is instantiated with a base Ubuntu image that has a few languages and runtimes pre-installed. To further customize the experience, a development container can be created that has all of the prerequisites installed, the proper versions of those prerequisites, and anything else that a team might need in order to compile/test the code. The concept of a development container (aka dev container) is not necessarily new; you can use a development container in your local instance of VS Code with Docker (more info on using dev container here). What is new, though, is running this directly in your browser!Yes you read right - right in your browser! A compute instance powers the developer’s environment, allowing for all development through a virtualized VS Code window in the browser! You can optionally connect your Codespace to your local VS Code if desired. There’s a toggle on the GitHub Codespaces main page that lets you see how the Codespace would look in the browser vs. desktop - and they are identical*.* if you have the proper configuration setup and synced as mentioned in this post ;)Setup VS Code Settings SyncBefore we configure Powerlevel10k, we need to make sure we set up VS Code settings sync. Even before I started at GitHub, I used my GitHub account to sync my settings. You could alternatively use a Microsoft account, but I think it makes more sense in this case to use a GitHub account since we will be launching a GitHub Codespace.One of the things we need to make sure this is setup for is for the Terminal font that I have defined for the Powerlevel10k theme (MesloLGS NF), but you’d want your other VS Code settings to sync as well.After firing up your Codespace, it should automatically sign you in and sync your settings and extensions, but if not, sign in manually.Configure Powerlevel10kThere are a few steps:1. Create a dotfiles repositoryNow, we need to create a dotfiles repository - and it needs to be public. GitHub knows to use the dotfiles repository created under your username. For example, here is my dotfiles repository.Bonus: I’ve cloned this repository locally and created a symlink from ~/.zshrc to ~/dotfiles/.zshrc. I followed this article, but I know others who have used the dotbot tool.The steps can be summarized by:git clone https://github.com/joshjohanning/dotfiles ~/dotfilesmv ~/.zshrc ~/.zshrc/dotfilesln -s ~/dotfiles/.zshrc ~/.zshrc2. Add your .zshrc and .p10k.zsh to your dotfiles repositoryAdd in your .zshrc and .p10k.zsh files to this repository!My .zshrc and .p10k.zsh are linked, respectively.If you followed something similar to the symlink example above, adding your .zshrc and .p10k.zsh file could be as simple as doing: git add .; git commit -m \"adding dotfiles\"; git push3. Update your .zshrc fileYou’re .zshrc likely hard codes your local user directory for the oh-my-zsh installation. Update it as such:# Path to your oh-my-zsh installation.export ZSH=\"/home/joshjohanning/.oh-my-zsh\"# Path to your oh-my-zsh installation.export ZSH=\"${HOME}/.oh-my-zsh\"4. Create an install.sh file to install Zsh theme and pluginsNow, we need to make sure our Powerlevel10k theme and Zsh plugins are installed when the Codespace is initialized.My install.sh script that I use that includes the Powerlevel10k setup is below:#!/bin/shzshrc() { echo \"===========================================================\" echo \" cloning zsh-autosuggestions \" echo \"-----------------------------------------------------------\" git clone https://github.com/zsh-users/zsh-autosuggestions ${ZSH_CUSTOM:-~/.oh-my-zsh/custom}/plugins/zsh-autosuggestions echo \"===========================================================\" echo \" cloning zsh-syntax-highlighting \" echo \"-----------------------------------------------------------\" git clone https://github.com/zsh-users/zsh-syntax-highlighting.git ${ZSH_CUSTOM:-~/.oh-my-zsh/custom}/plugins/zsh-syntax-highlighting echo \"===========================================================\" echo \" cloning powerlevel10k \" echo \"-----------------------------------------------------------\" git clone --depth=1 https://github.com/romkatv/powerlevel10k.git ${ZSH_CUSTOM:-$HOME/.oh-my-zsh/custom}/themes/powerlevel10k echo \"===========================================================\" echo \" import zshrc \" echo \"-----------------------------------------------------------\" cat .zshrc &gt; $HOME/.zshrc echo \"===========================================================\" echo \" import powerlevel10k \" echo \"-----------------------------------------------------------\" cat .p10k.zsh &gt; $HOME/.p10k.zsh}# change time zonesudo ln -fs /usr/share/zoneinfo/America/Chicago /etc/localtimesudo dpkg-reconfigure --frontend noninteractive tzdatazshrc# make directly highlighting readable - needs to be after zshrc lineecho \"\" &gt;&gt; ~/.zshrcecho \"# remove ls and directory completion highlight color\" &gt;&gt; ~/.zshrcecho \"_ls_colors=':ow=01;33'\" &gt;&gt; ~/.zshrcecho 'zstyle \":completion:*:default\" list-colors \"${(s.:.)_ls_colors}\"' &gt;&gt; ~/.zshrcecho 'LS_COLORS+=$_ls_colors' &gt;&gt; ~/.zshrc The cat .zshrc &gt; $HOME/.zshrc and cat .p10k.zsh &gt; $HOME/.p10k.zsh lines here are pretty important - this is what takes the content you have in your dotfiles repository and move it to the $HOME directory of the Codespace. I also wanted the machine to have my local time zone. Whenever I would commit, I would see UTC time in my git log. The GitHub UI translates this just fine, but my Jekyll blog theme uses the git commit timestamp when displaying the updated timestamp on the blog post, which I did not like since it was inconsistent with the posted time zone (where I’m using Central US). The zshrc line launches the zsh command prompt when I launch my CodeSpace instead of bash. And finally, when I would ls or use the directory autocomplete suggestions, I would see a lime green blackground with blue text on directories which was unreadable. These lines remove the highlighting and simply use a distinct color for the directories instead: Directories are unreadable with default zsh configuration Directories readable again! Important: Don’t git add this just yet! Continue to the next step.This install.sh script is based on this post.5. Mark the install script as executable with gitKind of annoying, but if you don’t do this, you’ll notice that in your Codespaces creation logs that the install.sh script is not executable:git add install.sh --chmod=+x Note: After you run this command, you still might see that the install.sh file has a change that wants to be added/committed (viewed in the Source Control window in VS Code or with git status). Ignore or discard those changes (git restore install.sh).If you’ve already added it, you can remove it and re-add it with:git rm --cached install.shgit add install.sh --chmod=+x There’s an alternative command you can run to mark the file as executable in-place with git update-index --chmod=+x install.sh, but if you do that, every time you change the file the executable bit will get flipped off and you’ll have to run that command again. Inevitably, you will forget, and your Codespace’s Zsh environment will be broken.You can view the Codespaces creation logs by opening the command palette (CMD/CTRL + Shift + P) and typing &gt; Codespaces: View Creation Log6. Link your dotfiles repo to CodespacesGo to your GitHub Codespaces settings and make sure the ‘Automatically install dotfiles’ box is checked.7. Set zsh as the default terminal in CodespacesBy default, Codespaces will open up a bash terminal window. We just did all of this work to pimp out our Codespace, we should make sure it loads the zsh terminal by default instead. Add this line to your VS Code settings.json file by opening the command palette (CMD/CTRL + Shift + P) and typing &gt; Preferences: Open Settings (JSON) :\"terminal.integrated.defaultProfile.linux\": \"zsh\"This is an extended snippet of the relevant section in my VS Code’s settings.json : \"terminal.integrated.shell.osx\": \"/bin/zsh\", \"terminal.integrated.defaultProfile.linux\": \"zsh\", \"terminal.integrated.fontFamily\": \"MesloLGS NF\",Note the font configuration!8. Log into VS Code Settings Sync in the CodespacesAfter firing up your Codespace, sign into VS Code Settings sync (I use GitHub_)Gotchas Fonts - I was lucky as it seems that whatever configuration I had in my .p10k.zsh file and my font choice for my VS Code terminal (MesloLGS NF) seemed to work out of the box - but I could imagine some headache if you used a more custom font. You can selectively not sync certain settings, so if you want a more default font to be used in your Codespace and a custom font to be used locally, you could probably do so. If you are using the Brave Browser, the shield (AdBlock) functionality tends to show a degraded view of the terminal window. Flip that shield off for this site. Annoyingly, you have to do this for each Codespace you create, as there is not the ability to whitelist a subdomain - but there is a GitHub issue made for it. Full error with shields on for those interested: Error loading webview: Error: Could not register service workers: NotSupportedError: Failed to register a ServiceWorker for scope (‘https://1c1b9171-108f-4374-9efc-20593a07163b.vscode-webview.net/stable/ccbaa2d27e38e5afa3e5c21c1c7bef4657064247/out/vs/workbench/contrib/webview/browser/pre/’) with script (‘https://1c1b9171-108f-4374-9efc-20593a07163b.vscode-webview.net/stable/ccbaa2d27e38e5afa3e5c21c1c7bef4657064247/out/vs/workbench/contrib/webview/browser/pre/service-worker.js?id=1c1b9171-108f-4374-9efc-20593a07163b&amp;swVersion=2&amp;extensionId=vscode.markdown-language-features&amp;platform=browser&amp;vscode-resource-base-authority=vscode-resource.vscode-webview.net&amp;parentOrigin=https%3A%2F%2Fjoshjohanning-pipeline-templates-6497vrprh5r7v.github.dev’): The user denied permission to use Service Worker.. SummaryTake a look at our awesome development environment, all running from within our browser!Powerlevel10k ZSH theme in GitHub CodespacesAnd yes…I did write and test this blog post completely with GitHub Codespaces :)." }, { "title": "Working Azure DevOps Pipeline Caching for Angular CI", "url": "/posts/azdo-angular-pipeline-caching/", "categories": "Azure DevOps, Pipelines", "tags": "Azure DevOps, Pipelines, Angular", "date": "2021-11-14 20:30:00 -0600", "snippet": "OverviewI’ve tried several times to implement the Pipeline Cache task using Microsoft’s documentation, but have failed every time. I seemed to configure everything like the documentation indicates for my Node.js/npm (Angular) build, but the results are very inconclusive - I didn’t really be saving any CI time.For builds where the npm install takes 30-60 seconds…it’s not really a problem. However, recently I was working with a team where the npm install was taking 10 (!!!) minutes. This was not going to work for me, and for my sanity, I had to get this pipeline caching figured out.npm install taking 10 minutes to runHow To - The ExplanationI was finally able to figure out what I was missing, part in thanks to this post from High Performance Programmer - in particular, their screenshot:Cache task configuration in a Classic Build DefinitionNote how this differs from Microsoft’s documentation:variables: npm_config_cache: $(Pipeline.Workspace)/.npmsteps:- task: Cache@2 inputs: key: 'npm | \"$(Agent.OS)\" | package-lock.json' restoreKeys: | npm | \"$(Agent.OS)\" path: $(npm_config_cache) displayName: Cache npmNotice how the first screenshot is caching the $(Build.SourcesDirectory)/Project/node_modules folder vs Microsoft’s code sample is caching $(Pipeline.Workspace)/.npm - quite a critical difference! It makes sense after thinking about it, when you run npm install locally, where is it going to download all of the modules to? The node_modules folder in the root of the project, of course.The way the task works is it zips up and saves the path you specify and stores it to the build (as a build-in post-build step). During the next build, if the key matches, it downloads the zip and extracts it to the aforementioned path. Both of the above examples use the key: npm | “$(Agent.OS)” | $(Build.SourcesDirectory)/Project/package-lock.json, where it matches the OS the build is running on as well as the hashed content of the package-lock.json file.This means, that if you flip a build from Windows to Ubuntu, the key won’t match, and the contents of the cache won’t be restored. Likewise, if the hash of the package-lock.json file changes (ie: you add a package, change a package version, remove a package, etc.), the cache won’t be restored. In both cases, you would expect a full npm install from scratch. If the build completes successfully, you should expect a new cache to be uploaded as an automatically added post build step:Uploading of the cache as a post-job stepHow To - Just the YAMLPutting it all together, here’s what my task looks like:- task: Cache@2 displayName: load npm cache inputs: key: npm | $(Agent.OS) | $(Build.SourcesDirectory)/Source/MyWeb/package.json restoreKeys: | npm | \"$(Agent.OS)\" path: $(Build.SourcesDirectory)/Source/MyWeb/node_modules cacheHitVar: CACHE_HIT Note: You’ll notice my example is using package.json and not package-lock.json - the team I was working with wasn’t using the package-lock.json file, so I just wanted to illustrate that you can also use the package.json as a key and it will work just as well Note: The cacheHitVar set to CACHE_HIT will evaluate to true if the cache hit is a success - could be useful for a conditional task where maybe you don’t even run the npm install command at allGotchas There’s no way to delete a cache once it’s stored in the pipeline - you can simply change the key: property by adding another string literal - see the below example: # from: key: npm | $(Agent.OS) | $(Build.SourcesDirectory)/Source/MyWeb/package.json # to: key: npm | node_modules | $(Agent.OS) | $(Build.SourcesDirectory)/Source/MyWeb/package.json Branches - the caches are isolated between branches - meaning that if I create a feature branch off main, I won’t be able to use main’s cache - more info on this here Pull request runs do not write cache to the source or target branch, only the intermediate branch such as refs/pull/1/merge - more info on this here Expiration - the cache expires after 7 days of no activity (hint - create a scheduled build if you want to ensure that a poor soul doesn’t have to experience a 20 minute build on Monday morning)I also like this Medium post from Dev Shah for additional gotchas.SummaryBefore working with us, the team’s build averaged 30 minutes. Using a slimmed down build job and running using hosted agents brought us from 30 minutes to 20 minutes.Since we have added and properly configured the Pipeline Cache task in our Angular CI build, we have shaved off 10 minutes from each build. When our npm build alone takes 10 minutes, our average build time of 20 minutes has been reduced by 50% to 10 minutes:Average build time of 20 minutes shaved down to 10 minutes after adding the Cache taskThis was a huge win for the us and dev team, and I’m happy to say the third time’s the charm for me on trying to configure the Azure DevOps Pipeline Caching Task." }, { "title": "Azure Front Door Standard/Premium Preview - Tips, Tricks, and Lessons Learned", "url": "/posts/azure-frontdoor-preview-experience/", "categories": "Azure, Front Door", "tags": "Azure, Azure Front Door", "date": "2021-10-01 16:30:00 -0500", "snippet": "OverviewI want to talk about Azure Front Door - not the old Azure Front Door - the new Azure Front Door, the new PREVIEW Front Door with the Standard/Premium SKUs. But Josh, wait, this is a DevOps blog, why are you talking about Azure Front Door? Well, I had the pleasure experience of working with Front Door (Preview) on my most recent project, and thought I would be doing the world a disservice by not sharing a little bit of the frustration knowledge I have gained while working with it. Whether no one else is using this service or no one is talking about it, we struggled to find many resources online for how to do certain things in the Front Door (Preview), so this is where this article comes into play.I am not planning on writing an entire how-to article, this is just intended to serve as a resource that hopefully the SEO Gods can help make someone else’s life easier. If you are unfamiliar with Azure Front Door (Preview), or want some official background and guidance from Microsoft, see the What is Azure Front Door Standard/Premium (Preview)? page.Note: For the purposes of this article, I am going to abbreviate Azure Front Door as AFD, and when I say AFD, I mean the new Preview Azure Front Door; I will not be referring to the classic / old Front door here.(PS: Guinness Book of Records, see above for my submission on most times “Front Door” has been used in a single sentence)Things That Work Well Private Endpoints Azure Front Door does a great job of routing to services such as App Services, Function Apps, Storage Accounts, and Private Link Services that are protected via Private Endpoints Since these are ‘magic’ aka managed Private Endpoints, the Private Endpoint doesn’t live in your subscription and you don’t have access to it. Therefore, there doesn’t seem to be a way to get the Origin Group / Origin deployment to automatically approve these, so you have to remember to go to the target resource and approve the Private Endpoint manually. This is similar to how Azure Data Factory’s Private Endpoints work Private Endpoints only work with the Premium SKU Certificates! Azure Front Door does a great job of automatically managing certificates - including expirations - the default setting is to let Azure Front Door handle all of this for you with 0 configuration You still have the option to bring your own certificate by creating an Azure Front Door Secret linked to a Certificate in an Azure Key Vault - Azure Front Door even shows expiration of that certificate on the Domain page If you are using your own certificate, it needs to be in PFX / PKCS 12 format (not PEM) Web Application Firewall (WAF) Anecdotally, I only have experience with the Premium SKU of the Azure Front Door Preview service, but creating a WAF tied to Microsoft-provided default rule set is relatively simple The Standard SKU does not let you use a WAF Letting teams share a single Front Door resource Teams can manage their own Endpoints in a single Azure Front Door resource without worry of mucking it up too much for other teams This splits the current $165/monthly cost for the Premium SKU Linking Origin to just about any hostname Works great for serving static websites hosted in an Azure Storage Account - we created a static website container $web and set our Origin to Origin Type: Custom and Host Name: mystaticsite.z21.web.core.windows.net We got the idea from article as a basis for this static website setup (okay this is referencing the Classic Azure Front Door, but the custom host name screenshot was what did the trick) If using Private Endpoints, you still have to have Azure Front Door create a private endpoint to the storage account. We set the ‘target sub resource’ (aka groupId in the ARM/API) to web. We did something similar with our Kubernetes linkage, creating a Private Link Service bound to the AKS managed subnet and Internal Load Balancer of our nginx service. In this pattern, the Origin Type: Custom and Host Name was set to the IP of the Internal Load Balancer and we used null() for our Origin Host Header and let nginx do header routing based on the URL that is being sent from Front Door to the Cluster. We set the ‘target sub resource’ (aka groupId in the ARM/API) to null(). See below example Works as expected with App Services and Function Apps; just set Origin Type: App Services Things That Don’t Work Well Private Endpoints For about 3-4 weeks in July/early August, Azure Front Door’s Private Endpoints just completely died and Microsoft support was super slow in getting any attention to this. I get that it’s a Preview feature and things happen, but the resolution time was a little disappointing. We haven’t had any issues since they “reverted” the change that broke this, though You have to manually approve the Private Endpoint on your target resource URL Rewriting We were trying to use a single Azure Front Door Endpoint to host all of our App Service APIs as Origins; sort of like a poor man’s APIM (around 18x cheaper if you’re not using all of the premium features of a Premium APIM) We wanted myfd.z01.azurefd.net/foo to redirect to foo.azurewebsites.net Maybe we were just doing it wrong, but we struggled to do native URL Rewriting in Azure Front Door - it’s incredibly possible likely that we are just misinterpreting how it’s supposed to work See my Stack Overflow post of what we were trying to do, and someone’s suggestion on how to resolve (I have not had a chance to test yet) We instead did URL Rewriting by using middleware at the app level. It just requires a making a small modification in the startup.cs file (and if you’re serving a Swagger page, there too!). See: examples below For Function Apps, there is no way to rewrite the incoming URL. However, you can edit the host.json file and customize the base path by modifying the routePrefix property. See: example below Update Times Okay minor gripe, but it takes anywhere from 5-20 minutes for a change you make to Front Door to propagate down to you Sometimes loading in a different browser / using a proxy can help alleviate cache/dns issues No native Terraform Resource (yet) We are using the azurerm_template_deployment resource to deploy an ARM template within Terraform Editing via the UI You have to click the ‘Edit’ button on the Endpoint, then click into the Origin Group/Route to make changes - if you just click on the Origin Group/Route without clicking ‘Edit’ first, you will just be in a read only mode. WebSockets Azure Front Door does not support WebSockets - use Long Polling instead for SignalR use cases Random notes HTTPS Redirect in .NET Web Apps It is considered best practice to redirect http to https. This can be done in the code by adding the following to the startup.cs file: app.UseHttpsRedirection(); The problem with this method when using Azure Front Door with Private Endpoints is that this causes the app to redirect to the host (azurewebsites.net) instead of the incoming host URL (custom domain on Azure Front Door) To work around this, you should remove this line of code altogether from the application and let Front Door redirect traffic to HTTPS (a setting on the Route) If you’re working with an Angular app, make sure to remove any HTTPS redirect from the web.config Deleting Endpoints / Domains / Front Door If you want to delete a domain, you will need to clean up all of the associations (i.e.: the Route) You can delete the entire Endpoint the domain is associated to as well If there is/was a WAF associated to that endpoint, though, you need to manually go into the WAF resource and remove the association to the domain manually. This isn’t made clear by the UI error (Failed to delete the custom domain(s)) or the CLI error ((BadRequest) Property 'AfdDomainEntityKey.AfdDomainName' cannot be set to 'mysubdomain.mydomain.com'.) If you go to re-create the Endpoint with the same name, note that it will fail for the first time with a Conflict: That resource name isn't available error message!!! You simply have to attempt to create the endpoint another time and then it will go through properly. The error will look something like this: error\": {\\r\\n \"code\": \"Conflict\",\\r\\n \"message\": \"That resource name isn't available.\" If you delete your entire Front Door, you still need to delete the WAF manually, and you will still see a conflict error message the first time you try to re-create a deleted endpoint Our services aren’t available right now error Make sure you have approved the Private Endpoint on the target resource Alternatively, go and edit the Origin Group &gt; Origin, uncheck the Private Endpoint box, save the Origin, Save the Origin Group, wait 10-30 seconds for it to apply, edit the Origin Group &gt; Origin, check the Private Endpoint box and select the right resource, save the Origin, save the Origin Group, and go and re-approve the Private Endpoint on the target resource &lt;h2&gt;Our services aren't available right now&lt;/h2&gt;&lt;p&gt;We're working to restore all services as soon as possible. error Your endpoint is still provisioning Or, your Route is misconfigured The HTML will be not be rendered on the page for this error - if it does render it means Front Door is routing correctly it’s likely a problem with a private endpoint (see above) Page not found blue page error Wait 5-20 minutes for the Endpoint to provision Sometimes CORS errors disguise themselves as a misconfigured Endpoint / Private Endpoint Be familiar with grabbing a Bearer token to interact directly with the Azure REST APIs As an example, you can plug that Bearer token into Postman and use this GET request to list the details about a Origin in an Origin Group: GET https://management.azure.com/subscriptions/xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx/resourceGroups/my-afd-rg/providers/Microsoft.Cdn/profiles/my-afd/originGroups/myorigingroup/origins?api-version=2020-09-01&amp;Full This can also be helpful when deciphering what changes / values you need to use in an ARM template SummaryHopefully I haven’t scared you away; Azure Front Door (Preview) has a lot of great features and shows a lot promise! And when it works, it works real well. Half of the frustration that we had with this service was that no one else had written about it, so we were kind of making it up as we go along. We have a solid foundation now, and once there is a native Terraform module, making changes for us will be even easier.Happy hacking!See the Appendix below for miscellaneous logging, URL rewriting, AFD CLI, and ARM template examplesAppendix: ExamplesLog Analytics / Diagnostics QueryThis assumes you have a Diagnostics Settings created that captures FrontDoorAccessLog logs and sends to a Log Analytics resource.See the below for an example Log Analytics / Diagnostics query to find non-200 HTTP Status CodesAzureDiagnostics | where httpStatusCode_s != 200 and TimeGenerated &gt; ago(500m)URL RewriteSee the below sections for examples on how to do URL Rewriting in .NET (.NET Core) App Services and Function Apps.NET - startup.csusing Microsoft.AspNetCore.Rewrite; // add this usingpublic void Configure(IApplicationBuilder app, IWebHostEnvironment env, ILoggerFactory loggerFactory){ // url rewrite var options = new RewriteOptions().AddRewrite(@\"^myapi/(.*)\", \"$1\", skipRemainingRules: true); app.UseRewriter(options); ... // remainder of code below} Note: This makes the URL for local development something like http://localhost:5001/myapi/... Note: If you need /path and /path/ to work, then the regex for the rewrite should be @\"^myapi[/]?(.*)\".NET - SwaggerIf you are using Swagger for an API, you should also update the prefix for the Swagger endpoint:// Enable middleware to serve generated Swagger as a JSON endpoint.app.UseSwagger();app.UseSwaggerUI(c =&gt;{ // Specify swagger JSON endpoint var prefix = apiPath == \"\" ? \"\" : $\"/{apiPath}\"; c.SwaggerEndpoint($\"{prefix}/swagger/{apiVersion}/swagger.json\", apiDefinitionTitle); c.DocExpansion(DocExpansion.None); // specifying the Swagger-ui endpoint. c.RoutePrefix = \"swagger-ui\"; c.DefaultModelsExpandDepth(-1);});Function Apps - host.jsonhost.json:{ \"version\": \"2.0\", \"extensions\": { \"http\": { \"routePrefix\": \"function1\" } }AFD CLI CommandsDelete Origin Groupaz afd origin-group delete --profile-name my-afd --resource-group my-afd-rg --origin-group-name myorigingroup --yesDelete Custom Domainaz afd custom-domain delete --profile-name my-afd --resource-group my-afd-rg --custom-domain-name mysubdomain.mydomain.netDelete Routeaz afd route delete --profile-name my-afd --resource-group my-afd-rg --endpoint-name my-endpoint --route defaultDelete Endpointaz afd endpoint delete --profile-name my-afd --resource-group my-afd-rg --endpoint-name my-endpointShow Endpointaz afd endpoint show --profile-name my-afd --resource-group my-afd-rg --endpoint-name my-endpointAdding Custom Domain with CertificateNote that when --custom-domain-name asks for a name, it’s just the friendly name of the domain as it appears in AFDaz afd custom-domain create -my-afd-rg --custom-domain-name foobar --profile-my-afd --host-name '*.mysubdomain.mydomain.com' --minimum-tls-version TLS12 --certificate-type CustomerCertificate --secret my-wildcard-cert-pfx --debugPurging Cacheaz afd endpoint purge --ids \"/subscriptions/xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx/resourceGroups/my-afd-rg/providers/Microsoft.Cdn/profiles/my-afd/afdendpoints/my-endpoint\" --content-paths \"/*\"ARM Null() property and TerraformWe couldn’t figure out a way to pass in null() from Terraform to our ARM template using the azurerm_template_deployment resource, and passing in \"\" failed in mysterious ways or ended up just hanging the deployment. Essentially, we wrote some logic to convert the \"\" passed into the template parameter to null() for us.Note lines 20, 25, and 69 for the relevant logic:{ \"$schema\": \"https://schema.management.azure.com/schemas/2019-04-01/deploymentTemplate.json#\", \"contentVersion\": \"1.0.0.0\", \"parameters\": { \"profileName\": { \"type\": \"string\" }, \"originGroupsName\":{ \"type\": \"string\" }, \"hostName\":{ \"type\": \"string\" }, \"hostHeader\":{ \"type\": \"string\" }, \"privateLinkResourceId\":{ \"type\": \"string\" }, \"privateLinkResourceType\":{ \"type\": \"string\" } }, \"variables\": { \"is-private-link-service-type\": \"[equals('', parameters('privateLinkResourceType'))]\", \"is-null-host-header\": \"[equals('', parameters('hostHeader'))]\" }, \"resources\": [ { \"type\": \"Microsoft.Cdn/profiles/originGroups\", \"apiVersion\": \"2020-09-01\", \"name\": \"[concat(parameters('profileName'), '/', parameters('originGroupsName'))]\", \"properties\": { \"loadBalancingSettings\": { \"sampleSize\": 4, \"successfulSamplesRequired\": 3, \"additionalLatencyInMilliseconds\": 50 }, \"healthProbeSettings\": { \"probePath\": \"/\", \"probeRequestType\": \"HEAD\", \"probeProtocol\": \"Http\", \"probeIntervalInSeconds\": 100 }, \"sessionAffinityState\": \"Disabled\" } }, { \"type\": \"Microsoft.Cdn/profiles/originGroups/origins\", \"apiVersion\": \"2020-09-01\", \"name\": \"[concat(parameters('profileName'), '/', parameters('originGroupsName'), '/default')]\", \"dependsOn\": [ \"[resourceId('Microsoft.Cdn/profiles/originGroups', parameters('profileName'), parameters('originGroupsName'))]\" ], \"properties\": { \"hostName\": \"[parameters('hostName')]\", \"originHostHeader\": \"[if(variables('is-null-host-header'), null(), parameters('hostHeader'))]\", \"httpPort\": 80, \"httpsPort\": 443, \"priority\": 1, \"weight\": 1000, \"enabledState\": \"Enabled\", \"sharedPrivateLinkResource\": { \"privateLinkLocation\": \"[resourceGroup().location]\", \"privateLink\": { \"id\": \"[parameters('privateLinkResourceId')]\" }, \"groupId\": \"[if(variables('is-private-link-service-type'), null(), parameters('privateLinkResourceType'))]\", \"requestMessage\": \"Private link service from AFD\" } } } ]}" }, { "title": "Azure DevOps: Migrate Work Items to New Organization / Project", "url": "/posts/azure-devops-migrate-work-items/", "categories": "Azure DevOps, Work Items", "tags": "Azure DevOps, Work Items", "date": "2021-09-29 20:35:00 -0500", "snippet": "OverviewIf you have used Azure DevOps for a long time, you probably have asked / been asked if you can just simply move work items in one project to another. Maybe there was a company re-org, or the work was created in a ‘temporary’ project and needs a final resting spot, or you’re migrating to a new Azure DevOps organization for whatever reason. If you are vaguely aware of the tool, you’ll know that this can sometimes be easier said than done, especially if you want a migration with any level of fidelity. Not to say a full-fidelity migration is the end-all-be-all - sometimes the ask can be solved with considerably less effort if one was to just import the work items into the new project and if history is needed, simply refer to the old project. In like 3 weeks, the history will be meaningless anyways. In this post, I want to detail some options that you have as well as some caveats and gotchas.I want to highlight at a high level the options: Scenario Consideration Option(s) Moving work items to a project within the same organization Full fidelity Native ‘move work item’ tool in Azure DevOps Moving work items to a project in a different organization Not full fidelity - just need the work items Excel integration using Office Integration Tools (Windows-only) or CSV Import/Export (cross-platform) Moving work items to a project in a different organization Full fidelity nkdAgility/azure-devops-migration-tools or microsoft/vsts-work-item-migrator Moving work items to a project within the same organizationLucky you, this is the easiest. In the April 13 2016 sprint update (that didn’t make it to Azure DevOps Server until 2019!!), the Azure DevOps team added the ability to move a work item between projects within the same organization. As quoted from the sprint note: Users may now move a work item(s) between team projects. The work item ID remains the same and all of the work item’s revisions are moved. Users may also change type during a move and add a comment to be included as part of the work item’s discussion section.You can either move a single work item from the ellipses and selecting the Move to Team Project option. This pops up the work item in the new project and prompts you to fix any area / iteration path validation errors before saving.You can additionally write a query for the work items you want to migrate and multi-select (using ctrl-a, ctrl-click, ctrl-shift-click, etc.), right click, and select Move to Team Project…. From there, you will be brought to a pageMigrating multiple work items to a new projectThis method moves the work item in whole, preserving the original work item ID and history. If you move parent/child items (i.e.: Feature/User Story), these relationships are preserved as well. You even have the option to convert the items being moved to the new team project to a different work item type.There is more documentation of this feature on this page.Moving work items to a project to a different organization - no fidelity but easierWhen moving work items to a different organization, the task becomes a little harder. The easiest of the options is to use the Excel Office Integration Tools plugin (Windows-only) or CSV Import/Export (cross-platform). It’s relatively simple, but still a little more complex than just moving work items to a new project in the UI.There are two sub-options here, but both will require a query to be created.Assuming you want child/parent relationships migrated, create a tree query that includes all of the columns of data that you want to export. Here is an example list of columns: Work item name Work item type Assigned to State Iteration Path Area path Tags Description (text formatting might change slightly) Acceptance Criteria (text formatting might change slightly) Remaining Work Effort Backlog Priority Priority Value Area Business Value Time Criticality Target Date Fields not migrated with this method: Any formatting in the description field should get exported as HTML, including pasted images, but the img src will still be the source project Original creator Date created Attachments History Other links in work items i.e.: related to, affected by, changeset, hyperlinks… Excel Office Integration Tools Create matching area paths / iteration paths Create the query with the source work items and the columns/fields you want to export Create the same query in the target project. I like to use the Wiql Editor to be able to easily copy/paste the query to the target project, sort of like a copy/pasting a SQL query Open up the source query in Excel, using the ‘New List’ button under the Team ribbon Open up a new Excel sheet and load the target query. The target query should be empty; it just should have all of the columns in the same order This seems to work better in new instance of Excel vs. new tab in same sheet Use the ‘Add Tree Level’ button to create the same number of ‘Title’ columns that the source query has - for example, if migrating Epics, Features, User Stories, Tasks, you would have Title 1, Title 2, Title 3, Title 4 in your source query Copy all of the content - except the work item ID - from the source query table into the target query table Fix any ‘Assigned To’ names, Area Paths, Iteration Paths, etc. Here’s a slightly tricky part - the work items have to be saved as New (or To Do for a task with the Scrum template) Excel will show a validation error for work items that have a non-new state Replace each state with ‘New’ or the new equivalent Assuming there are no more validation errors, ‘Publish’ the work items and wait (tip: I’ve found that any more than 1000 at a time and Excel will start to choke up) Now that the work items are published, do not refresh! You want them in the same exact list as you’ve copied them in - you can then return to your original query and copy the entire state column and paste it on top of the state column in the target table Publish again - the imported work items should have their original states nowExport/Import CSV Create matching area paths / iteration paths Create the query with the source work items and the columns/fields you want to export Export the query as CSV - click the ellipses in the upper right and ‘Export to CSV’ Open the CSV and remove all of the Work Item IDs from the Work Item ID column Modify the CSV to change any fields that you want to ahead of time, such as area path / iteration paths In the target project, go to Boards &gt; Work Items - there is an ‘Import Work Items’ button Import the CSV and resolve any validation errors Excel will show a validation error for work items that have a non-new state Replace each state with ‘New’ or the new equivalent (i.e.: ‘To Do’ for Tasks in the Scrum template) You can use the Shift key select multiple work items, right click, and edit these items in bulk Once all the validation errors have been resolved, click the ‘Save Items’ button in the upper left To put the original states back, from this same screen, click the ‘Export to CSV’ button - the CSV exported here with the new Work Item IDs should be in the same order as the query that was originally exported/imported. You can then return to the original CSV and copy the entire state column and paste it on top of the state column in the newly exported CSV ‘Import from CSV’ the new CSV that has the new Work Item IDs and the original state ‘Save Items’ again - the imported work items should have their original states nowMoving work items to a project to a different organization - full fidelity but harderThere are a couple tools to do this, but the tool that I have the most experience in is the nkdAgility/azure-devops-migration-tools tool. Microsoft has a tool, Microsoft/vsts-work-item-migrator, but I have not used.If you are going to go down this route, I recommend checking out Martin’s video about how this works and how you can configure the tool. Since the video was posted, the configuration and ‘Processors’ have changed slightly. Enabling and modifying the processor settings is how you configure the various components you want migrated. There is additional documentation for the tool on this page, and more information specifically on the processors on this page.It’s a great tool and it works really well, but one thing I found difficult when I got started was getting a sample configuration file to use. Now, I haven’t used the migrator tool in a while, but I do have a v11 configuration file that I have used. Specifically, I have used version 11.9.34.My sample configuration file is found in this gist.Search for the string \"Enabled\": and you will find the different processors that you can enable. In the sample configuration, the only enabled processor is the WorkItemMigrationConfig processor. You will need to modify the WIQLQueryBit to meet your needs. In the example highlighted below, the WIQL is going to migrate all User Stories, Tasks, Features, Epics, Bugs, and Test Cases - but not Test Suites and Test Plans (these are migrated in a the TestPlansAndSuitesMigrationConfig processor). If you were only migrating work items under an area path, you would add another clause, such as AND [System.WorkItemType] UNDER \"MyArea/Path\".Example: { \"$type\": \"WorkItemMigrationConfig\", \"Enabled\": true, \"ReplayRevisions\": true, \"PrefixProjectToNodes\": false, \"UpdateCreatedDate\": true, \"UpdateCreatedBy\": true, \"BuildFieldTable\": false, \"AppendMigrationToolSignatureFooter\": false, \"WIQLQueryBit\": \"AND [System.WorkItemType] IN ('User Story', 'Task', 'Feature', 'Epic', 'Bug', 'Test Case') AND [System.WorkItemType] NOT IN ('Test Suite', 'Test Plan')\", \"WIQLOrderBit\": \"[System.ChangedDate] desc\", \"LinkMigration\": true, \"AttachmentMigration\": true, \"AttachmentWorkingPath\": \"c:\\\\temp\\\\WorkItemAttachmentWorkingFolder\\\\\", \"FixHtmlAttachmentLinks\": false, \"SkipToFinalRevisedWorkItemType\": true, \"WorkItemCreateRetryLimit\": 5, \"FilterWorkItemsThatAlreadyExistInTarget\": true, \"PauseAfterEachWorkItem\": false, \"AttachmentMaxSize\": 480000000, \"CollapseRevisions\": false, \"LinkMigrationSaveEachAsAdded\": false, \"GenerateMigrationComment\": true, \"NodeBasePaths\": [ ], \"WorkItemIDs\": null }Run MigrationHere is a high-level list of steps that you need to do in order to run the migration: Add the ReflectedWorkItemId field to each of the work items you are migrating in both the source and target project. I believe you can get away without adding it to the source project, but then it makes it impossible to re-start a migration Install the version you want choco install vsts-sync-migrator (oh yeah, this only runs on Windows only the last I checked) Or download from the releases page; I have most recently used version 11.9.34 Set up your configuration file - my sample configuration file is found in this gist I usually run the Area Path processor (TfsTeamSettingsProcessorOptions, which referencesTeamSettingsSource and TeamSettingsTarget) by itself first to make sure the area path / iteration nodes are created. Decide if you want to enable PrefixProjectToNodes, where it will prefix the source project name in the area/iteration structure Command to run migration and save the log to file: migration.exe execute -c configuration.json &gt; log.txtThe migrator has pretty good output logging, just search the output log file for error to work through any errors you have.Pro-tip on running the migration: If you can, run this inside a virtual machine inside of Azure so you have the best possible internet capabilities and don’t have to worry about your computer falling asleep.Old, bust maybe useful notes: I have a lot of notes in an old README.md, but it’s from v7.5 circa summer 2019, so it’s probably not super relevant, but including here in case anyone can glean anything from it. There are some additional explanations on what the various setting are for the WorkItemMigrationConfig. I also have an old configuration file from v8.9.x that might be helpful to some - I had a lot of success with it in July 2020.Other Useful Scripts Azure DevOps - Create Iterations PowerShell Script (but you should be able to use this for areas as well, just replace /Iterations with /Areas)ConclusionMigrating work items can be easy but can easily be made complicated if things such as history and attachments are a requirement. I mean, who really looks at the work item’s history anyway? I am a bigger fan of migrating the work items as is and leaving the work items in a read-only state in the source project that can be referred to for some time if history is needed.But obviously, sometimes this isn’t possible, so that’s why an article like this exists! I hope someone finds this helpful down the line. Feel free to leave any additional tips that you’ve found in the comments, or feel free to reach out for additional strategies you are considering. Good luck!" }, { "title": "GitHub Actions: Publish Code Coverage Summary to Pull Requests", "url": "/posts/github-code-coverage/", "categories": "GitHub, Actions", "tags": "GitHub, GitHub Actions, Pull Requests, Code Coverage", "date": "2021-09-08 22:00:00 -0500", "snippet": "OverviewThis is a follow-up to my previous post: The Easiest Way to Generate and Publish .NET Code Coverage in Azure DevOpsI was familiar with adding Code Coverage to my pipelines in Azure DevOps and having a Code Coverage tab appear on the pipeline summary page, but I wasn’t sure what was available for GitHub Actions. With GitHub Actions really starting to pick up steam, especially with recent additions such as Composite Actions, I thought now would be a great time to explore. Update: Since May 9, 2022, you can also post the Code Coverage to the Job Summary page! See this in action here, and my implementation example here.Adding Code Coverage Report to GitHub ActionsI found this GitHub Action in the marketplace - Code Coverage Summary. There might be others, but this one seemed simple and had the functionality I was looking for.This post assumes you are using the coverlet.collector NuGet package. For a refresher, see the “the better way” section of my previous post.Here’s the relevant part of my .github/workflows Action file: # Add coverlet.collector nuget package to test project - 'dotnet add &lt;TestProject.cspoj&gt; package coverlet - name: Test run: dotnet test --no-build --verbosity normal --collect:\"XPlat Code Coverage\" --logger trx --results-directory coverage - name: Copy Coverage To Predictable Location run: cp coverage/*/coverage.cobertura.xml coverage/coverage.cobertura.xml - name: Code Coverage Summary Report uses: irongut/CodeCoverageSummary@v1.0.2 # uses: joshjohanning/CodeCoverageSummary@v1.0.2 with: filename: coverage/coverage.cobertura.xml badge: true format: 'markdown' output: 'both' - name: Add Coverage PR Comment uses: marocchino/sticky-pull-request-comment@v2 if: github.event_name == 'pull_request' with: recreate: true path: code-coverage-results.mdNote the test command here that we are using to generate the Cobertura code coverage summary file:dotnet test --no-build --verbosity normal --collect:\"XPlat Code Coverage\" --logger trxWe have to use a copy command to copy the coverage.cobertura.xml to a known location - the marketplace action we are using doesn’t seem to support wildcards and Coverlet uses a random guid folder path.cp coverage/*/coverage.cobertura.xml coverage/coverage.cobertura.xmlThe next action is the Code Coverage Summary Report action: CodeCoverageSummary Inputs: filename: coverage/coverage.cobertura.xml badge: true | false format: markdown | text output: console | file | both - name: Code Coverage Summary Report uses: irongut/CodeCoverageSummary@v1.0.2 with: filename: coverage/coverage.cobertura.xml badge: true format: 'markdown' output: 'both'This would be enough to show the code coverage in the action run: Code Coverage Summary Report in the Action run logsHowever, the fun doesn’t stop there. How useful would it be to post this to the PR so it’s nice and easy for reviewers? Well, the next action shows a simple way we can add (and sticky) a PR comment with our code coverage report: - name: Add Coverage PR Comment uses: marocchino/sticky-pull-request-comment@v2 if: github.event_name == 'pull_request' with: recreate: true path: code-coverage-results.mdPerfect - nothing for us to configure here, either. On the pull request, this comment is added:Code Coverage Summary Report added as a pinned comment to the Pull RequestThis is also demonstrated on my pull request here.You’ll notice the badge along with the markdown table summarizing the code coverage report.Also, if a new commit is pushed to the PR, triggering a new action run, the comment will be deleted/re-added with the updated code coverage summary.ReportGenerator?The above works well if you have a single test project, but if you have more than one, see the “Why not ReportGenerator? section in my previous post for the commands and rationale.The equivalent in GitHub Actions would be: - name: Create code coverage report run: | dotnet tool install -g dotnet-reportgenerator-globaltool reportgenerator -reports:$(Agent.WorkFolder)/**/coverage.cobertura.xml -targetdir:$(Build.SourcesDirectory)/CodeCoverage -reporttypes:'Cobertura'ConclusionMaybe not as pretty as the Cobertura report shown in Azure DevOps, but just as effective!And hey, now on the GitHub Pull Request, you get to actually see the code coverage report before the end of the entire pipeline run like in Azure DevOps :)." }, { "title": "The Easiest Way to Generate and Publish .NET Code Coverage in Azure DevOps", "url": "/posts/azure-devops-code-coverage/", "categories": "Azure DevOps, Pipelines", "tags": "Azure DevOps, Code Coverage", "date": "2021-09-03 04:00:00 -0500", "snippet": "OverviewPublishing code coverage in Azure DevOps and making it look pretty is way harder than it should be. It’s something that sounds simple, oh just check the box on the task - but nope you have to make sure to read the notes and add the additional parameter to the test task. Okay great, now you have a code coverage tab, but what is this .coverage file and how do I open it? That’s not very user friendly. And don’t get me started on having to wait for the entire pipeline to finish before you can even see the code coverage tab - nonsensical.If you want to navigate to the solution, scroll down.Not Good: The Out of the Box WayIf using the out of the box dotnet task with the test command, simply add the publishTestResults argument (or if using the task assistant, check the Publish test results and code coverage checkbox):.NET Core Task Assistant UIHowever, if you read the information on the publishTestResults argument from the .NET Core CLI task (or clicking on the (i) on the Publish test results and code coverage option in the task assistant), it says: Enabling this option will generate a test results TRX file in $(Agent.TempDirectory) and results will be published to the server.This option appends --logger trx --results-directory $(Agent.TempDirectory) to the command line arguments. Code coverage can be collected by adding --collect \"Code coverage\" option to the command line arguments. This is currently only available on the Windows platform.Emphasis: mine. So even if you check the box, you need to ensure you add the --collect \"Code coverage\" argument. Oh, and you have to run this on a Windows agent, so no ubuntu-latest for us.This produces code coverage that looks like the following in Azure DevOps:How the default code coverage in Azure DevOps looksIt’s a link to a .coverage file..which is great if you 1) have Visual Studio installed and 2) are on Windows (can’t open .coverage file on Mac).The Better WayThe better way is to add the coverlet.collector NuGet package to (each of) the test project(s) that you run dotnet test against.The easiest way to do this is to run the dotnet package add command targeting the test project:dotnet add &lt;TestProject.cspoj&gt; package coverlet.collectorAdding coverlet.collector with dotnet add packageFor those who can’t run the dotnet command, add the following under the ItemGroup block in the .csproj file: &lt;PackageReference Include=\"coverlet.collector\" Version=\"3.1.0\"&gt; &lt;IncludeAssets&gt;runtime; build; native; contentfiles; analyzers; buildtransitive&lt;/IncludeAssets&gt; &lt;PrivateAssets&gt;all&lt;/PrivateAssets&gt; &lt;/PackageReference&gt;See the following page for the latest version.Next, update the pipeline to ensure your dotnet test command looks like mine, and adding the PublishCodeCoverageResults@1 task. # Add coverlet.collector nuget package to test project - 'dotnet add &lt;TestProject.cspoj&gt; package coverlet.collector' - task: DotNetCoreCLI@2 displayName: dotnet test inputs: command: 'test' projects: '**/*.Tests.csproj' arguments: '--configuration $(buildConfiguration) --collect:\"XPlat Code Coverage\"' publishTestResults: true # Publish code coverage report to the pipeline - task: PublishCodeCoverageResults@1 displayName: 'Publish code coverage' inputs: codeCoverageTool: Cobertura summaryFileLocation: $(Agent.TempDirectory)/*/coverage.cobertura.xml # using ** instead of * finds duplicate coverage filesThe --collect:\"XPlat Code Coverage\" argument is what tells dotnet test to use the coverlet.collector package to generate us a cobertura code coverage report. As you can guess by the XPlat in the argument, this runs cross platform on both Windows and Ubuntu.This argument creates a $(Agent.TempDirectory)/*/coverage.cobertura.xml code coverage report file. This folder is default output folder since Azure DevOps adds --results-directory /home/vsts/work/_temp to the command.Next, we have to specifically add the PublishCodeCoverageResults@1 task to publish the code coverage output to the pipeline. It seems like at least with my project, it produces 2 coverage.cobertura.xml files and that throws a warning in the pipeline, so that’s I used $(Agent.TempDirectory)/*/coverage.cobertura.xml not $(Agent.TempDirectory)/**/coverage.cobertura.xmlDuplicate coverage.cobertura.xml code coverage resultsNow, after the entire pipeline has finished (including any of the deployment stages), we will have a code coverage tab with a way more visually appealing code coverage report:Cobertura Code Coverage Report in Azure DevOpsWhy not ReportGenerator?I’ve seen many blog posts that are similar to mine, except that they have the reportgenerator@4 task. I used to think this was required, too! But I have recently found out it is not - at least not if you have ONE test project you are publishing results for.If you have multiple test projects you would like code coverage published for, then yes, the reportgenerator@4 task is needed.I have started to use the command line instead of the actual reportgenerator@4 task itself, though, as not every organization has the marketplace extension installed (and some have stricter policies about adding extensions than others). # First install the tool on the machine, then run it - script: | dotnet tool install -g dotnet-reportgenerator-globaltool reportgenerator -reports:$(Agent.WorkFolder)/**/coverage.cobertura.xml -targetdir:$(Build.SourcesDirectory)/CodeCoverage -reporttypes:'HtmlInline_AzurePipelines;Cobertura' displayName: Create code coverage reportreportgenerator@4 equivalent: # ReportGenerator extension to combine code coverage outputs into one - task: reportgenerator@4 inputs: reports: '$(Agent.WorkFolder)/**/coverage.cobertura.xml' targetdir: '$(Build.SourcesDirectory)/CoverageResults'This needs to run before the PublishCodeCoverageResults@1 task, and that task needs to be updated just a little bit (the xml file path is different). See: # Publish the combined code coverage to the pipeline - task: PublishCodeCoverageResults@1 displayName: 'Publish code coverage report' inputs: codeCoverageTool: 'Cobertura' summaryFileLocation: '$(Build.SourcesDirectory)/CoverageResults/Cobertura.xml' reportDirectory: '$(Build.SourcesDirectory)/CoverageResults'Code Coverage Tab Not Showing Up?This doesn’t really solve the problem where Azure DevOps will not show the Code Coverage tab until the entire pipeline (including all deployments) has completed. In cases where code coverage is important, I either: Change the report output from $(Build.SourcesDirectory) to $(Build.ArtifactStagingDirectory) and publish the report as a pipeline artifact. Create a separate build pipeline than deployment pipeline. This harkens back to the day where we didn’t have multi-stage YAML builds and we had separate Build Definitions and Release Definitions. We can set up the deployment yml pipeline to be triggered from the completion of the build yml pipeline.ConclusionNow you know the ins and outs of adding Code Coverage to your .NET (Core) projects in Azure DevOps.Stay tuned to my next post on what we can do with code coverage in GitHub Actions!" }, { "title": "Azure DevOps: No agent pool found with identifier xx", "url": "/posts/agent-pool-error/", "categories": "Azure DevOps, Pipelines", "tags": "Azure DevOps, Pipelines", "date": "2021-08-18 08:00:00 -0500", "snippet": "OverviewWe are using the Virtual Machine Scale Set (VMSS) Azure DevOps agents pretty heavily. They are perfect for our situation, needing to be able to deploy to services locked down by private endpoints while not having to individually manage agent installation/configurations.I re-created a VMSS to use a different .vhd image, and thought I had to delete/re-create the agent pool in Azure DevOps. I learned afterwards this isn’t the best way, the best way is to just edit the existing agent pool and point to your Azure Subscription –&gt; VMSS again to re-configure it.I had deleted agent pools within a project no problem before, but this time, I actually wasn’t the one that had originally created this pool. I went to go re-create the agent pool with the exact same name and received this error message: No agent pool found with identifier 59.I suspected I was able to delete the agent pool from the project because I was a Project Administrator, but I wasn’t able to delete from the organization/collection since 1) I wasn’t the creator of the agent pool, they are assigned special rights and 2) I wasn’t a Project Collection Administrator.However, I was surprised to find that I couldn’t even see the agent pool in the list. https://dev.azure.com/ORG/_settings/agentpoolsI tried querying the REST API and it didn’t appear there.Since I didn’t know any of the Project Collection Administrators in this organization, my solution was to ask the original creator to go to the Organization –&gt; Agent Pools settings to delete the agent pool so I could re-create it.Lesson learned - don’t delete, just edit :). But not a super helpful error message from Azure DevOps’s part." }, { "title": "Azure DevOps: Pipeline Templates", "url": "/posts/pipeline-templates/", "categories": "Azure DevOps, Pipelines", "tags": "Azure DevOps, Pipelines, Pipeline Templates", "date": "2021-08-12 23:00:00 -0500", "snippet": "OverviewAlthough linked in various posts in this blog, I never fully advertise my pipeline-templates GitHub repository. I refer back to this every so often to see how I accomplished something in the past with various pipeline YAML concepts and will often share to others when they are in need of an example to follow.Link: https://github.com/joshjohanning/pipeline-templatesIn each folder, you will typically see the build as well as the deployment yaml template. I also started including the azure-pipelines-*.yml file to show an example of how to consume/reference the pipeline templates.Let me know if there are any questions or areas for improvement!" }, { "title": "Azure DevOps: Delete Custom Fields on Process Template", "url": "/posts/azdo-delete-custom-field/", "categories": "Azure DevOps, Work Items", "tags": "Azure DevOps, Work Items", "date": "2021-08-10 23:30:00 -0500", "snippet": "OverviewI had the annoying misfortune today of running into an issue in Azure DevOps when customizing a process template. I added a field to a work item but I created the field as the wrong type. Once the custom field is created, there is not way to delete the field through the UI. Even with the REST API, it was a little tricky to find.Deleting the FieldFirst: You’ll need to be a Project Collection Administrator in order to run this API.This is a link to the API we are going to use.DELETE https://dev.azure.com/{organization}/{project}/_apis/wit/fields/{fieldNameOrRefName}?api-version=6.0In Postman, let’s create a new request with that URL string. I would have thought we would have passed in the Work Item Process Template Name instead of the Project, but I suppose it knows based on the project what process template it is using.Here is an example for my Azure DevOps organization / team project deleting the NewTestField.https://dev.azure.com/jjohanning0798/TestTeamProject/_apis/wit/fields/NewTestField?api-version=6.0Additionally, we should create a Personal Access Token (PAT) with full permissions.Under the Postman authentication’s tab, we can leave the username blank and enter the PAT for the password. Use Basic Authentication.Setting up Basic authorization in Postman with a PATSend the request.If it was successful, you will see a 204 No Content message near theSuccessful request in PostmanThe field should no longer appear on our work item, and we can re-create the field with the right name and type.If you don’t have the proper permissions (ie: Project Collection Administrator), you’ll receive the following message:\"message\": \"Access Denied: 08dd71ec-5369-619d-bc32-495207cd99b7 needs the following permission(s) on the resource Delete field from organization to perform this action: Delete field from organization\",Enjoy!" }, { "title": "Tokenizing Angular Environment Configuration in Azure DevOps", "url": "/posts/angular-tokenization/", "categories": "Azure DevOps, Pipelines", "tags": "Azure DevOps, Pipelines, Angular", "date": "2021-06-17 18:30:00 -0500", "snippet": "OverviewI was working with a team that had an Angular front-end application and I was tasked with improving their CI/CD process. They had some automated pipelines, but they were running a build before each environment by running a different npm run build -- --prod --configuration &lt;env&gt; command.My co-worker Colin Dembovsky summarizes it well in a similar post for .NET Core: The Build Once Principle If you’ve ever read any of my blogs you’ll know I’m a proponent of the “build once” principle. That is, your build should be taking source code and (after testing and code analysis etc.) producing a single package that can be deployed to multiple environments. The biggest challenge with a “build once” approach is that it’s non-trivial to manage configuration. If you’re building a single package, how do you deploy it to multiple environments when the configuration is different on those environments?Basically, if you’re running a new build for each environment, you might as well not do any tests after your Dev build because there’s no way you can guarantee that the binaries build for Dev are the same as the ones going into Production. Never mind that it’s inefficient and wastes time - you already compiled your code once, why do it again? The only things that should differ between environments should be the environment-specific configuration (such as a connection string, or in my case, the back-end API url).I’m taking the concepts from that post and my experience with a few other clients and will be doing something very similar to that here!The ProblemThe particular challenge with Angular is that the build output is not the same file name every time - you can’t just swap in a new file with the proper values. See screenshot for th main.js files from two builds:Compiled main.js output from two different buildsSolution and File EditsWe are going to make a few modifications and additions to the Angular code, but most of the changes will come in the pipeline.Pre-requisites: Qetza’s Replace Tokens extension installed in the Azure DevOps organizationangular.jsonYour angular.json file might look a little different, but what I did was take an existing configuration, copy/paste it, and change the fileReplacements section, specifically the src/environments/environment.tokenized.ts line. \"configurations\": { ... \"tokenized\": { \"fileReplacements\": [{ \"replace\": \"src/environments/environment.ts\", \"with\": \"src/environments/environment.tokenized.ts\" }], \"optimization\": true, \"outputHashing\": \"all\", \"sourceMap\": false, \"namedChunks\": false, \"extractLicenses\": true, \"vendorChunk\": false, \"buildOptimizer\": true, \"budgets\": [{ \"type\": \"initial\", \"maximumWarning\": \"2mb\", \"maximumError\": \"4mb\" }, { \"type\": \"anyComponentStyle\", \"maximumWarning\": \"100kb\", \"maximumError\": \"150kb\" } ] } } }environments.tokenized.tsSimilarly, I copied an existing src/environments/environments.*.ts file to create the environments.tokenized.ts file that my new configuration will use. The important line here is the baseUrl: '#{baseUrl}#' line. Notice how I’m creating a token here with the #{token-name}# syntax. This is the syntax of the token that our deployment process will know to find and replace.import { NgxLoggerLevel } from 'ngx-logger';export const environment = { production: true, baseUrl: '#{baseUrl}#', webUrl: location.origin, loggerLevel: NgxLoggerLevel.OFF};azure-pipelines.ymlFinally, we just need to modify our pipeline file! I’ll break down the changes in chunks.1. NPM BuildIn the Build job, update the NPM build command. In this example, the command this task will produce will be npm run build -- --prod --configuration=tokenized. Alternatively, you may just opt to use a script task, but the Npm@1 task also works.- task: Npm@1 displayName: \"npm build\" inputs: command: custom workingDir: src verbose: false customCommand: run build -- --configuration=tokenized2. Add Variables for your TokensFor each token you have, add a like-named variable. The task uses the variable name/value to find/replace in the tokenized file. Here’s an example of setting a variable at the stage level (ie: Deploy to Dev), but the variable could also be scoped at the job level. Just note that the variable is created without the #{ prefix or }# suffix - in other words, just create the variable as the token name without the wrappings.- stage: deployDev displayName: \"Deploy Dev\" variables: baseUrl: https://mysite-dev.azurewebsites.net/3. Extract FilesI’m assuming your build artifacts are published to the Pipeline, which is going to create a zip. In the Deployment job, we need to unzip the artifact before we can inject the real values in place of the tokens.- task: ExtractFiles@1 displayName: 'Extract files' inputs: archiveFilePatterns: '$(pipeline.workspace)/**/*.zip' destinationFolder: '$(Pipeline.Workspace)/deploy' Unzip Note 1: If you are running on your own ubuntu agents, make sure unzip is installed first! Unzip Note 2: Similarly, if you are running on your own agents, the above example requires the workspace to be cleaned for each run otherwise on the second run it will find more than one .zip file to extract. You could alternatively use a stronger typed pattern than the **/*.zip pattern in finding your zips, of course.4. Replace TokensRight after you extract the contents of the artifact zip, add in the Replace Tokens task. Note how my rootDirectory parameter is the same as the destinationFolder parameter from the unzip task. Additionally, targetFiles parameter is using a pattern to find the main*.js file, no matter what the file gets named for each build.- task: qetza.replacetokens.replacetokens-task.replacetokens@3 displayName: 'Replace tokens' inputs: rootDirectory: '$(pipeline.workspace)/deploy' targetFiles: '**/main*.js' escapeType: none verbosity: detailed Replace Tokens Note 1: Normally I don’t use the full name when referencing pipeline tasks, but if you also have Colin’s ALM Corner Build and Release Tools, you’ll run into an ambiguous task name error. Replace Tokens Note 2: If you opted to not use the default token prefix/suffix like #{token-name}#, you can add in the tokenPrefix and tokenSuffix parameters here as well. Further documentation is here. Replace Tokens Note 3: If you had tokens in other .js files, you could simply use a **/*.js pattern.5. Azure Web App DeployAssuming your deploying this Web App to Azure, update your task to use the new folder location instead of the zip package. The input takes either a zip or a folder, so we could have zipped our folder back up after running the replace tokens task, but there is no need. We simply need to use the same path for package that we used above for destinationFolder and rootDirectory.- task: AzureWebApp@1 displayName: 'Deploy Azure Web App' inputs: azureSubscription: MyAzureSubscription appType: 'webAppLinux' appName: WebAppName package: '$(Pipeline.Workspace)/deploy'SummaryThe build job will use our tokenized configuration to run the build and use our #{baseUrl}# token to use when compiling instead of a Dev or Prod URL. When you build locally, you can still use whatever other configuration you want without having to worry about the tokenized value. Just be sure to keep your config in sync, that is if you add or change something to one environment*.ts file, make sure to remember to do the tokenized one as well.The deployment job will… take your published artifact zip from build extract it use the variable with the same name as the token to inject the value in for the right deployment environment deploy your web app like normal! rinse and repeat for all of your environmentsReplace Tokens output in the pipelineBuild Configuration / File Replacement Update 08/12/2021I saw this in an angular.json file and had to update this post. This might be a more elegant solution than creating an entirely new tokenized build configuration: \"configurations\": { \"production\": { \"fileReplacements\": [ { \"replace\": \"apps/My-Angular-App/src/environments/environment.ts\", \"with\": \"apps/My-Angular-App/src/environments/environment.prod.ts\" } ], ... } }This encapsulates the best of both worlds - we are still building once and deploying many, but we also don’t need a specialized build configuration to run through. We can use the normal production build configuration and file replace the tokenized environments.prod.ts with environment.ts at build time.The deployment replace tokens task will replace the tokens with the proper environment-specific variable configuration." }, { "title": "So You Want to Migrate Trac Tickets to GitHub Issues", "url": "/posts/trac-to-github/", "categories": "GitHub, Migrations", "tags": "GitHub, GitHub Issues", "date": "2021-01-25 08:30:00 -0600", "snippet": "DisclaimerI should first off state that I wouldn’t entirely recommend this, if you are migrating to GitHub for the first time, you should try to start off with a blank slate. Keep the old tickets in Trac or the database around for a period in time in case you need to reference, but don’t migrate the entire Trac ticket repository. I could understand wanting to port in active tickets, though, which is a very valid use case for a tool like I am going to demonstrate.We were working with a client who was migrating off of their old Trac server, and I wanted to document (mostly for myself, but for anyone else who finds this too) how exactly it worked.ToolsThere are plenty of tools out there on GitHub (svigerske/trac-to-github, robertoschwald/migrate-trac-issues-to-github, hershwg/github-migrate-trac-tickets… some of them require XML-RPC, which I had a heck of a time installing on my Apache Trac webserver, so I wasn’t able to test those.The two I have tested are: mavam/trac-hub (joshjohanning/trac-hub) trustmaster/trac2githubBoth of these are more focused on Issues, and neither really do attachments (see trac-hub how to section below for a possible solution though). The svigerske/trac-to-github tool mentions that it uploads attachments as gists.Note: These tested against GitHub’s Cloud instance (not server).trac-hubtrac-hub OverviewI originally tested out trac2github and was going to write about that, but I think I might like this trac-hub tool a little better. When comparing the two, trac-hub might be better in that it uses the Import Issues API. The issues import API never left preview, so be aware it’s not officially supported from GitHub. The benefits of using this API are that it can create issues: without hitting abuse detection warnings and getting blocked without sending email notifications without increasing your contribution count to ridiculous heights much faster than with the normal issues API with correct creation/closed date set atomically without users being able to interfere in the creation of a single issueBasically, using this tool allows the GitHub issue to look like it was created originally when it was created in Trac versus looking like a brand new issue that was created.I created a fork of mavam/trac-hub (joshjohanning/trac-hub) (that has since been merged) that adds the ability to map Trac ticket owners to GitHub Issues assignees. Make sure not to typo the GitHub username as the issue will fail to create if a ticket’s owner (assignee) has a mapping in the configuration file. If a mapping doesn’t exist, the GitHub Issue will be created with no assignee, as expected.The caveats of both of the tools is that the “Issue creator” will appear as the one who originally ran the tool - but at least with this tool, we can preserve create and comment dates.trac-hub How ToI ran this on a debian 10.7 server, the same server that was running my Trac installation.Pre-requisites to install: sudo apt-get install bundler sudo apt-get install libmariadb-dev sudo apt-get install libsqlite3-devInstructions: Clone the repository - git clone https://github.com/joshjohanning/trac-hub Rename the example configuration file - mv config.yaml.example config.yaml Edit the configuration file vim config.yaml and modify the following sections: trac to provide the sqlite database path or mysql connection (untested) github to provide the target org/repo name and personal access token (make sure to grant the token enough access!) users to provide a list of mapping of Trac users –&gt; GitHub. If a mapping doesn’t exist, it won’t use the GitHub handle and just refer to the user as the Trac user. In my version of the tool, if a mapping is found but the GitHub handle doesn’t exist, it won’t migrate tickets –&gt; issues that are owned by/assigned to that non-existant user. Essentially, just make sure that the mapping uses a valid GitHub user or don’t include it :) labels to provide a mapping of Trac ticket metadata to labels in GitHub Issues Run the migration in a test repo! sudo bundle exec trac-hub -v -s 1 -F. Command line options: -v : verbose/debug logging -s 1 : start at the the first ticket in Trac -F : fast import, import without safety-checking the issue number. The way this tool runs is it expects Trac ticket #1 to be created as Issue #1. If you already have issues in the repository, or you want to do testing, add the -F import. For your real migration, you could probably drop this and trac ticket #1 will map to issue #1. Otherwise, with this -F argument, the issues will be created even though the ID’s won’t make 1:1. No example here, but -a for attachment-urls is interesting. The tool doesn’t migrate attachments, but if you used one of the download-trac-attachment-*.sh scripts in the repo, you could host the files somewhere, presumably with the same file names and it will hyperlink the attachments?? trac-hub Command Line Argument List$ bundle exec trac-hub --help -c, --config config set the configuration file -s, --start-at ID start migration from ticket with number &lt;ID&gt; -r, --rev-map-file FILE allows to specify a commit revision mapping FILE -a, --attachment-url URL if attachment files are reachable via a URL we reference this here -S, --single-post Put all issue comments in the first message. -F, --fast-import Import without safety-checking issue numbers. -o, --opened-only Skips the import of closed tickets -v, --verbose verbose modetrac-hub Example Migration ScreenshotsMigrating 5 tickets from Trac:Original ticket list in TracHow they appear in GitHub:Issues in GitHub migrated using trac-hubTrac ticket comments:Original ticket comments in TracIssue comments:Issue comments in GitHub migrated using trac-hubtrac2githubtrac2github OverviewThis was the first tool that I used, and it works! It uses the regular GitHub Issues API, which is subject to rate limits and abuse detection. The team that tried to use this import with me ran into the abuse detection a few times and tweaked lines 484 and 485 of the php script to play more friendly with GitHub (decreasing $request_count &gt; 50) and increasing sleep(70). Note that these are only used if $ticket_limit is set in the configuration file.The team did not like this tool because the original create date was not preserved - all of the issues look like they were created at the time the import tool ran.Otherwise, it does the job at migrating tickets, comments, user mapping with assignee, and label mapping.trac2github How ToI ran this on a debian 10.7 server, the same server that was running my Trac installation.Pre-requisites to install: PHP (e.g. on Ubuntu/Debian sudo apt-get install php) (I prefer sudo apt-get install php-fpm to NOT install a new version of apache over my existing web server) Support for the trac database format, e.g. sudo apt-get install php-mysql, sudo apt-get install php-sqlite3, etc. sudo apt-get install php-curlInstructions: Clone the repository - git clone https://github.com/joshjohanning/trac-hub Edit the configuration file vim config.yaml and modify the following variables: $username : GitHub username b. $password GitHub personal access token (make sure to grant the token enough access!) c. $project : The GitHub organization you are migrating to. If migrating issues to your own GitHub repo under your own account, this would be the same as $username. Use the Organization name if using GitHub Enterprise $repo : The GitHub repo name $users_list : Provide a mapping of Trac user –&gt; GitHub user The database driver settings - whether that’s $mysqlhost_*, $sqlite_trac_path, or $pgsql_ settings. Explore the other items in the config file to see if they are needed, such as $ticket_offset for resuming a migration or $remap_labels to modify the label mapping. Note that while you might not think you need to set $ticket_limit because you want to migrate the entire Trac ticket database, this setting needs to be set in order to trigger the aforementioned rate limiting sleep control. Therefore, I advise giving $ticket_limit an arbitrary value for this purpose. Run the import: php trac2github.phpYou’ll notice that this import is a lot slower than trac-hub :).trac2github Example Migration ScreenshotsMigrating 5 tickets from Trac:Original ticket list in TracHow they appear in GitHub:Issues in GitHub migrated using trac2githubTrac ticket comments:Original ticket comments in TracIssue comments:Issue comments in GitHub migrated using trac2githubTakeawaysRun a few of these migrations, and run them in repositories that you don’t mind deleting afterwards as deleting issues can be…challenging.Run the migration using someone’s PAT that you don’t mind being the creator for the Issues (throwaway / service account possibly??).Make sure you have the users created in GitHub and mapped in the appropriate configuration file ahead of time.Be patience, have reasonable expectations, and good luck!" }, { "title": "Azure DevOps: Bulk Reparent Work Items", "url": "/posts/reparent-work-items/", "categories": "Azure DevOps, Work Items", "tags": "Azure DevOps, Work Items, Scripts", "date": "2021-01-17 16:30:00 -0600", "snippet": "Reparent Work Items in the UIIf you are reparenting only a few work items, then the easiest way is to use the Mapping view in the Azure DevOps backlog, as described by Microsoft:Enabling the mapping paneReparenting work items in Azure DevOps with the mapping paneReparent Work Items with a ScriptHowever, mapping (or reparenting) work items in the Azure DevOps UI can be a little clunky - it can be done in mass using the parent mapping pane, but what if you have hundreds or thousands of work items split across multiple parent/child relationships or multiple backlogs? Then it becomes harder since you can’t use this functionality in a query window, only from the backlog.This is a very simple PowerShell script utilizing the Azure DevOps CLI extension that can very quickly update the parent on a query of work items. I used a combination of the CLI commands with PowerShell, since PowerShell makes it super simple to use loops and JSON parsing. Before the Azure DevOps CLI, this would have to have been done with using the APIs, which isn’t hard, but this solution uses way fewer lines of code!In this example, I am using a Tag on the work items I want to update.################################ Reparent work item################################ Prerequisites:# az devops login (then paste in PAT when prompted)[CmdletBinding()]param ( [string]$org, # Azure devops org without the URL, eg: \"MyAzureDevOpsOrg\" [string]$project, # Team project name that contains the work items, eg: \"TailWindTraders\" [string]$tag, # only one tag is supported, would have to add another clause in the $wiql, eg: \"Reparent\" [string]$newParentId # the ID of the new work item, eg: \"223\")az devops configure --defaults organization=\"https://dev.azure.com/$org\" project=\"$project\"$wiql=\"select [ID], [Title] from workitems where [Tags] CONTAINS '$tag' order by [ID]\"$query=az boards query --wiql $wiql | ConvertFrom-JsonForEach($workitem in $query) { $links=az boards work-item relation show --id $workitem.id | ConvertFrom-Json ForEach($link in $links.relations) { if($link.rel -eq \"Parent\") { $parentId=$link.url.Split(\"/\")[-1] if($parentId -ne $newParentId) { write-host \"Unparenting\" $links.id \"from $parentId\" az boards work-item relation remove --id $links.id --relation-type \"parent\" --target-id $parentId --yes write-host \"Parenting\" $links.id \"to $newParentId\" az boards work-item relation add --id $links.id --relation-type \"parent\" --target-id $newParentId } else { write-host \"Work item\" $links.id \"is already parented to $parentId\" } } }}Improvement Ideas Utilize the environment variable or from file method to be able to run az devops login in an unattended fashion Use the APIs if you are so inclined, but I still like to use the CLI when possible Expand the WIQL, and maybe add it as a script parameter!" }, { "title": "Authorize and Restore Azure Artifacts NuGet Packages in GitHub Actions", "url": "/posts/authorize-azure-artifacts-in-github-actions/", "categories": "GitHub, Actions", "tags": "Azure DevOps, NuGet, GitHub, GitHub Actions, Azure Artifacts, Artifactory, CodeQL", "date": "2021-01-04 14:15:00 -0600", "snippet": "SummaryWhile this post is geared towards Azure DevOps and Azure Artifacts, this approach will work for any third-party feed that requires authentication (like Artifactory!).I needed to be able to restore my NuGet packages hosted in an Azure Artifacts instance in a GitHub Action workflow. In Azure Pipelines, it’s relatively simple with the Restore NuGet Packages task. This task dynamically creates a NuGet config with the proper authentication details to Azure Artifacts. In GitHub Actions, there isn’t a native action readily available for us to accomplish this.I tried the shubham90-nugetauth marketplace action, but I couldn’t get it to work. The inputs only called for the azure-devops-org-url, not a particular Artifact feed, so I wasn’t sure how it sets up the configuration for the feeds since an organization could have multiple NuGet feeds. There is another action that looks promising, GitHub NuGet Private Source Authorisation, but I decided to use the command line for increased flexibility.What I did instead was borrow some of my scripting knowledge from my NuGet Pusher post to programmatically add my Azure Artifacts feed as a source (with credentials) and restore the solution. This is summarized with a few simple commands:- name: Auth NuGet run: dotnet nuget add source ${{ env.nuget_feed_source }} --name ${{ env.nuget_feed_name }} --username az --password ${{ secrets.AZURE_DEVOPS_TOKEN }} --configfile ${{ env.nuget_config }} - name: Restore NuGet Packages run: dotnet restore ${{ env.solution_file_path }}Notes: This should work with either .NET Core as well as full .NET Framework on both Linux and Windows The --configfile argument is optional - if not specified, there is a hierarchy involved: It will first try to use the NuGet.config in the current working directory first Next, it will use the local user NuGet.config in %appdata%\\NuGet\\NuGet.Config (Windows) or ~/.nuget/NuGet.Config (Linux/Mac, depending on distro) If the .sln is in the root (or current working directory), you can simply run dotnet restore without the solution path as well Reference the dotnet nuget add source and dotnet restore docs for more informationSetupLet’s take a step back and add some things that are necessary to make this work. First, we have to generate an Azure DevOp Personal Access Token to Next, we have to create a secret either at the repository or GitHub organization with an Azure DevOps PAT that has access to the Artifact feed. I called my secret: AZURE_DEVOPS_TOKEN For reusability and ease, let’s add in a few environment variables to the GitHub Action workflow:env: solution_file: 'My.Core.App.sln' nuget_feed_name: 'My-Azure-Artifacts-Feed' nuget_feed_source: 'https://pkgs.dev.azure.com/&lt;AZURE-DEVOPS-ORGANIZATION&gt;/_packaging/&lt;MY-AZURE-ARTIFACTS-FEED&gt;/nuget/v3/index.json' nuget_config: '.nuget/NuGet.Config'Note that my Azure Artifacts feed was scoped to the Organization level, the NuGet Feed Source url will be slightly different depending on if you used a Project feed. The source URL can be found by navigating to the Azure Artifacts feed and clicking the Connect to Feed button.Complete WorkflowIncluding the complete code scanning workflow for reference - the only bit custom here is the environment variables and the Auth NuGet and Restore NuGet Packages run commands:name: \"CodeQL\"on: push: branches: [ main ] pull_request: branches: [ main ] schedule: - cron: '19 0 * * 2' workflow_dispatch: # manual triggerenv: solution_file: 'My.Core.App.sln' nuget_feed_name: 'My-Azure-Artifacts-Feed' nuget_feed_source: 'https://pkgs.dev.azure.com/&lt;AZURE-DEVOPS-ORGANIZATION&gt;/_packaging/&lt;MY-AZURE-ARTIFACTS-FEED&gt;/nuget/v3/index.json' nuget_config: '.nuget/NuGet.Config' jobs: analyze: name: Analyze runs-on: windows-latest strategy: fail-fast: false matrix: language: [ 'csharp' ] steps: - name: Checkout repository uses: actions/checkout@v2 # Initializes the CodeQL tools for scanning. - name: Initialize CodeQL uses: github/codeql-action/init@v1 with: languages: ${{ matrix.language }} # # If exists, remove existing AzDO NuGet source that doesn't have authentication # - name: Remove existing entry from NuGet config # run: dotnet nuget remove source ${{ env.nuget_feed_name }} --configfile ${{ env.nuget_config }} - name: Auth NuGet run: dotnet nuget add source ${{ env.nuget_feed_source }} --name ${{ env.nuget_feed_name }} --username az --password ${{ secrets.AZURE_DEVOPS_TOKEN }} --configfile ${{ env.nuget_config }} - name: Restore NuGet Packages run: dotnet restore ${{ env.solution_file_path }} # Autobuild attempts to build any compiled languages (C/C++, C#, or Java). # If this step fails, then you should remove it and run the build manually (see below) - name: Autobuild uses: github/codeql-action/autobuild@v1 - name: Perform CodeQL Analysis uses: github/codeql-action/analyze@v1When would you have to use the dotnet sources remove command?You may have noticed a commented-out run command in the above workflow:# If exists, remove existing AzDO NuGet source that doesn't have authentication- name: Remove existing entry from NuGet config run: dotnet nuget remove source ${{ env.nuget_feed_name }} --configfile ${{ env.nuget_config }}If your NuGet.config already has an Azure DevOps entry, you will need to remove it with dotnet nuget remove source otherwise you will likely see 401 Unauthorized errors during the dotnet restore. This is because that entry doesn’t (or at least shouldn’t!) have any credentials associated with it committed into source control, so it essentially tries to access it anonymously and will fail.Also, we have to remove it because we cannot add a sources entry to the NuGet.config with the same name.Improvement Ideas / Notes If your solution does not contain a NuGet.config file, you may have to create a temporary config file similar to how the NuGet Command task works in Azure DevOps Alternatively, simply omitting the --configfile argument will use the local user NuGet.config Using the local NuGet.config will certainly work with GitHub-hosted runners since it’s a fresh instance each time, but you may run into conflicts if you’re on a shared self-hosted runner This marketplace action uses a local NuGet.config by default The Restore NuGet Packages command might not be needed since the Autobuild action performs a restore as well - therefore one may also be able to remove the solution_file variable - but I always like to have an explicit task for restoring packages so I know exactly if that failed If you the Autobuild Action does not successfully build your project for code scanning, you will have to build it manually. Using full .NET Framework, there is an additional action that you need to add to add MSBuild to the path (microsoft/setup-msbuild@v1.1). Also make sure to add the /p:UseSharedCompilation=false argument mentioned from Troubleshooting the CodeQL workflow: No code found during the build. Here’s an example:- name: Add msbuild to PATH uses: microsoft/setup-msbuild@v1.1- name: MSBuild Solution run: msbuild ${{ env.solution_file }} /p:Configuration=debug /p:UseSharedCompilation=falseArtifactoryI’ve seen a few instances where a team is using an API key to access Artifactory, so the command is slightly different:- name: Auth NuGet run: nuget setapikey admin:password -Source Artifactory - name: Restore NuGet Packages run: dotnet restore ${{ env.solution_file_path }}Notes: The -ConfigFile argument can optionally be used to specify a NuGet.config file Reference the nuget setapikey docs for more information" }, { "title": "Quickly Migrate NuGet Packages to a New Feed in Bulk", "url": "/posts/nuget-pusher-script/", "categories": "Azure DevOps", "tags": "Azure DevOps, NuGet, Scripts", "date": "2020-12-23 16:45:00 -0600", "snippet": "SummaryThis is a very simple bash script that can assist you in migrating NuGet packages to a different Artifact feed. It’s written with Azure DevOps in mind as a target, but there’s no reason why you couldn’t use any other artifact feed as a destination.I used the script after I ran a NuGet restore locally of a Visual Studio solution, found my .NuGet folder with all of the cached packages, placed my script in that folder, and ran it. This saved me a lot of time, migrating 102 packages while I grabbed a cup of coffee!echo -n \"NuGet feed name?\"read nugetfeedecho -n \"NuGet feed source?\"read nugetsourceecho -n \"Enter PAT\"read pat# adding to ~/.config/NuGet/NuGet.confignuget sources add -Name $nugetfeed -Source $nugetsource -username \"az\" -password $pat results=$(find . -name \"*.nupkg\")resultsArray=($results)for pkg in \"${resultsArray[@]}\"do echo $pkg nuget push -Source $nugetfeed -ApiKey az $pkgdoneImprovement Ideas The username doesn’t matter in this script since when using an Azure DevOps PAT, username is irrelevant. If one was pushing to a NuGet feed that required username authentication, I would presume you would add that as an input. One could also add a target folder as an input too. Also one could use a more elaborate input mechanism to the script…" }, { "title": "GitHub: Block Pull Request if Code Scanning Alerts Are Found", "url": "/posts/github-codeql-pr/", "categories": "GitHub, Advanced Security", "tags": "GitHub, GitHub Actions, Pull Requests, CodeQL, GitHub Advanced Security, Policy Enforcement, Branch Protection Rules", "date": "2020-12-16 20:00:00 -0600", "snippet": "OverviewAfter virtually attending GitHub Universe last week and watching the GitHub Advanced Security round-up and Catching vulnerabilities early with GitHub sessions, it got me thinking: How do I block a pull request from being merged if the scans detect issues? I didn’t think the GitHub Docs were incredibly straight forward on how this works.I knew how to configure a branch protection rule in GitHub that enforces things such as a GitHub Action or Azure DevOps stage completes successfully, but what about code scanning? How configurable is it?How ToIf you already have a Code Scanning workflow configured, skip to step #7. The first thing you need is a public repository (GHAS is free for public repos) or a private repository with the GitHub Advanced Security license enabled In the Security tab on the repository, navigate to ‘Code scanning alerts’ page I’m using the native ‘CodeQL Analysis’ workflow by GitHub - there are 3rd party analysis engines here too! Take a look at the workflow file - I didn’t need to make any changes, but one can modify the language matrix if you want/don’t want scans to run for a particular language There’s an Autobuild step here that works most of the time, but for some repositories I found I had to build my app manually - further reading on build steps for compiled languages Commit the workflow to your branch and merge it into Main - for best results we want an initial scan in the default branch before we test out the PR process Under the Settings tab in the repository, navigate to Branches Create a branch protection rule for your default branch - check the ‘Require status checks to pass before merging’ box If you used the GitHub CodeQL workflow, add the CodeQL status check and save the rule You don’t want the Analyze (javascript) status check; that just will show if the particular scan has completed, not that it found any vulnerabilities If you don’t see the CodeQL to add as a status check to the branch protection, it won’t appear as an option until you initiate at least one PR on the repository that triggers and completes the entire CodeQL scan (meaning all of the Analyze jobs have finished) - as of December 2021, this is still an issue. It is vaguely alluded to in this tidbit in the documentation - emphasis mine: When the code scanning jobs complete, GitHub works out whether any alerts were added by the pull request and adds the “Code scanning results / TOOL NAME” entry to the list of checks. After code scanning has been performed at least once, you can click Details to view the results of the analysis. Step #9 was the part I wasn’t originally confused on. The other entry (eg. Analyze (javascript)) is only the scan job for the corresponding language. It should succeed irregardless of if vulnerabilities are found. If it fails, the autobuild task might not be able to compile your code. The Understanding the pull request checks GitHub documentation summarizes well.After configuring the code scanning workflow and branch protection policy, you should be all set!Branch Protection Policy with the CodeQL status check configuredTesting It OutAnother thing the GitHub Docs do not do a good job of spelling out is that only Errors or a security severity level of High or Higher are going to fail the Pull Request status check. Warnings, out of the box, do not block the PR.Alright, so let’s introduce an error…does anyone know of an easy vulnerability we can put in our code? Well neither do I, but we don’t have to with the help of our friend, the Semmle vulnerability database (Note: The day before I wrote this, this link started redirecting to GitHub, but I found an archive.org link to use for the purposes of this demo).I’m going to use an incorrect suffix vulnerability. The easiest way to introduce this is to: 1) make sure javascript is in the language matrix in our CodeQL workflow like so: language: [ 'csharp', 'javascript' ] and 2) check in a simple .js file somewhere in the repository with the bad code:function endsWith(x,y) { let index = x.lastIndexOf(y); return x.lastIndexOf(y) === x.length - y.length;}Make sure to commit this in a branch because we want to test out the PR flow!Afterwards, create the PR and wait for the job to run:Pull Request that is blocked because of a code scanning vulnerability (note that I can still force merge since I am an Administrator)Success! Or, failure, just as we wanted - the pull request cannot be merged because the CodeQL status check failed, meaning it detected a vulnerability in the code.Once you fix the vulnerable code and re-push, all of the status checks will be successful and you are free to merge:Pull Request with passing status checks - no vulnerable code has been foundThe fact that GitHub gives this away for free for all public repositories is incredible! There is a licensing upcharge for Enterprises, but the setup is so simple and integrations so robust makes it well worth it (and we’re only scratching the surface!).Status Check Failure ConfigurationBy default, the status check will only fail if there is an Error or a security severity level of High or Higher High or Higher vulnerability - Warnings or a security level of Medium will not fail the status check. However, we can use the Control which code scanning alerts cause a pull request check to fail feature that was release in June 2021 to configure what alert level will fail the PR:Control which code scanning alerts cause a pull request check to failFor more information, see the documentation for Defining the severities causing pull request check failure.SummaryNot allowing code that introduces vulnerabilities to be merged in the PR process is crucial to ensuring the integrity of our code. Blocking a PR that contains a code vulnerability is essentially THE use case of GitHub Advanced Security - we’re able to see right on our PR in GitHub that there’s a vulnerability, with the deep linking and integration that you would expect. Finding out about the issue at PR time shortens the feedback loop - we don’t have to scramble before a production deployment if your security scan is occurring too late in the process.Even without the branch protection configured, the code scanning results will still show that CodeQL found a vulnerability, but without the branch protection we would be able to freely merge this into main:Pull Request that shows a code vulnerability found, but since there is no branch protection on this repo, we are free to mergeThis might be enough for some, but if we’re going to go through the exercise of creating a code scanning workflow, just as it’s a best practice to require at least one other approver on the PR before merging, we should require that there are no vulnerabilities being introduced as well. GitHub Advanced Security prides itself on limiting the signal vs noise ratio, so the chance of getting a ‘High’ vulnerability result that is a false positive is pretty minimal. And if you do get a false positive or a result you aren’t going to fix - just dismiss the alert.Happy (secure) coding!Extra Credit - Analyzing a SARIF FileI originally had this section in here to assist with blocking PR’s from results other than Errors (such as Warnings), but GitHub has implemented this feature now. I will leave the section below as it might be useful in its own right if you are interested in deep-diving or debugging your SARIF results file.Original Content - Analyzing a SARIF Results File ManuallyOkay, what if we were to have a repository with Terraform code and used the ShiftLeft Analysis marketplace code scanning workflow? Or, we used the native GitHub CodeQL workflow but want it to block merges when it finds any result, including warnings?Well, in the case of the ShiftLeft Analysis workflow, there is a config file that can be uploaded to the root of the repository to define some of this, but I haven’t played around much for this. For the GitHub CodeQL workflow, there is no fine-tuning configuration file that we can easily use (that I know of at this date).For this, I wrote a script that examines the .sarif and added another job to the workflow, like so: Detect-Errors: runs-on: ubuntu-latest strategy: fail-fast: false matrix: language: [ 'csharp', 'javascript' ] needs: - analyze steps: - name: Download Sarif Report uses: actions/download-artifact@v2 with: name: sarif-report - name: Detect Errors run: | repo=$(echo ${{ github.repository }} | awk -F'/' '{print $2}') results=$(cat $repo/results/${{ matrix.language }}-builtin.sarif | jq -r '.runs[].results[].ruleId') resultsArray=($results) echo \"${resultsArray[*]}\" errorCount=0 warningCount=0 noteCount=0 for var in \"${resultsArray[@]}\" do severity=$(cat $repo/results/${{ matrix.language }}-builtin.sarif | jq -r '.runs[].tool.driver.rules[] | select(.id==\"'$var'\").properties.\"problem.severity\"') echo \"${var} | $severity\" if [ \"$severity\" == \"warning\" ]; then let warningCount+=1; fi if [ \"$severity\" == \"error\" ]; then let errorount+=1; fi if [ \"$severity\" == \"note\" ]; then let noteount+=1; fi done echo \"\" echo \"Error Count: $errorCount\" echo \"Warning Count: $warningCount\" echo \"Note Count: $noteCount\" echo \"\" if (( $errorCount &gt; 0 )); then echo \"errors found - failing detect error check...\" exit -1 fi if (( $warningCount &gt; 0 )); then echo \"warnings found - failing detect warning check...\" exit -1 fiThis will fail if any findings were found, including warnings - modify the script as needed.Note that we also have to add an Upload Build Artifact step in the Analyze job, like so: - name: Upload Sarif Report to Workflow uses: actions/upload-artifact@v2 with: name: sarif-report-${{ matrix.language }} path: /home/runner/work/**/*.sarifDepending on the workflow, you may have to modify the path in the Upload task as well as the script. You can find out the relative path of the .sarif report by viewing the Actions’ logs.The entire workflow can be found in my GitHub branch.Because the .sarif produced by the ShiftLeft analysis is slightly different and by default doesn’t fail the job even with errors, I created a different workflow you can use to block pull requests if errors or warnings are found - see for example.Now just like we did above, we can modify our branch rule to require the “Detect-Errors” job to finish successfully, as this job will run successfully if there are no errors/warnings.Adding the new job to the required status check listPull Request that is blocked because of a ‘warning’ result found in the code scanning resultsI’m sure there is probably a better way to do this (using the API or GraphQL endpoint?). I know back in the LGTM / Semmle days, there was also a config file you could commit to the root of the repository to more precisely define rules. Either way, let me know in the comments if you have any other ideas or improvements!" }, { "title": "Azure DevOps: Extends Template with Build and Deployment Templates", "url": "/posts/extends-template/", "categories": "Azure DevOps, Pipelines", "tags": "Azure DevOps, Pipeline Templates", "date": "2020-12-10 22:00:00 -0600", "snippet": "ScenarioI had a client that wanted to integrate a secret scanning utility (among other checks) into the pipeline, and enforce this control. Colin Dembovsky (my co-worker) and I typically recommend creating and referencing job templates for each environment. Job templates are very flexible, allowing for re-use across an organization but still allowing for differences between applications through the parameters passed into the template.A very abbreviated example of this would look like:resources: repositories: - repository: templates type: github name: joshjohanning/pipeline-templates endpoint: joshjohanningstages:- stage: 'Build' jobs: - template: build.yml@templates parameters: buildConfiguration: 'release'- stage: deployDev jobs: - template: deploy.yml@templates parameters: environment: 'dev'- stage: deployProd jobs: - template: deploy.yml@templates parameters: environment: 'prod'However, there is nothing here that enforces a developer to use these templates - they could either write their own or just create their pipeline inline. This is where Extends comes into play!I remember when this was first announced in a sprint release note in December 2019, we tried it and couldn’t really get it to work the way we wanted to. But with the client’s requirements, this seemed like a perfect time to give it another shot. I wanted to reference a separate configuration repository where the secret scanning config would be stored without the developer having to worry or care about it, and we found a way to do just that using Extends.The CodeIn this demo scenario, my code is stored in GitHub, but this could work just as well with code in Azure Repos as well.azure-pipelines.ymlIn the root azure-pipelines.yml file, you’ll notice that the extends keyword is at the same level as trigger and resources. This was the tricky part - how does one use extends AND job templates? The approach is to use a steps template for the build where we want the extra steps injected, and for deployment we can use our job templates like normal. We will add an Environment check that ensures that the extends template is being used. If the Extends template isn’t used, the check fails and the deployment isn’t allowed.The deployment stages and jobs are defined in this file as well - this should look very familiar to regular deployment jobs except that they are being referenced as a parameter.trigger: - main resources: repositories: - repository: templates type: github name: joshjohanning/pipeline-templates endpoint: joshjohanningextends: template: secret-scanning/secret-scanning-extends.yml@templates parameters: buildSteps: # use steps template for build - template: secret-scanning/sample-build-steps.yml@templates parameters: whatToBuild: 'Hello world' deployStages: - stage: dev displayName: deploy to dev jobs: # use job templates as normal for deployment # bug using github as template repo?: must use ../ - template: ../secret-scanning/sample-deployment-job.yml@templates parameters: environment: github-secret-scanning-test-gate-dev - stage: prod displayName: deploy to prod jobs: - template: ../secret-scanning/sample-deployment-job.yml@templates parameters: environment: github-secret-scanning-test-gate-prodsecret-scanning-extends.ymlThe parameters passed into the extends template include a stepList type for the buildSteps and a stageList for the deployStages.The resource and - template: secret-scanning-steps.yml here is the configuration repository I was mentioning before. For your use case, you may not need this, you would just need the steps in - ${{ parameters.buildSteps }}.The build stage and job is defined in this file.parameters:- name: buildSteps # the name of the parameter is buildSteps type: stepList # data type is StepList default: [] # default value of buildSteps- name: deployStages type: stageList default: [] resources: repositories: - repository: secretscanning type: github name: joshjohanning/secret-scanning-config endpoint: joshjohanningstages:- stage: secure_buildstage displayName: 'Secure Build Stage' jobs: - job: secure_buildjob steps: - template: secret-scanning-steps.yml - ${{ parameters.buildSteps }}- ${{ parameters.deployStages }}sample-build-steps.ymlNot much crazy here - this is a steps template (as opposed to a job template). This is injected into the extends template in the - ${{ parameters.buildSteps }} line of code.parameters: whatToBuild: ''steps:- script: | echo \"${{ parameters.whatToBuild }}, here is where I do my build!\"sample-deployment-job.ymlThis is pretty vanilla as well - this job template is injected into the extends template in the - ${{ parameters.deployStages }} line of code.parameters: environment: '' pool: vmImage: 'ubuntu-latest'jobs:- deployment: deploy displayName: deploy pool: ${{ parameters.pool }} environment: ${{ parameters.environment }} strategy: runOnce: deploy: steps: - script: | echo \"deploy hello world\" displayName: deployConfiguration in Azure DevOpsThe Required YAML Template check is added to the environment just as an Approval would be:Adding the required template check to an environmentNote here if you are storing the code in Azure Repos - the example in this screenshot mentions project/repository-name. If the repository is in the same project, DO NOT include the project name in the path otherwise it won’t work.Now, if you try to deploy to an environment while not using this extends template, it fails:Fails the required template checkIf you click the 0/1 checks passed, it shows the check that failed and hyperlinks to the checks for that environment:More details on the failed required template checkOnce you properly use the extends template - success!Passes the required template checkConclusion and Next StepsThis ‘Required Checks’ concept works really well for environments that are defined ahead of time as a way to manage logical groupings of like deployments and add manual approval points.Did you know that you can also add these same type of required checks on Service Connections? Yep! Therefore, you can configure your production Azure Service Connection such that ONLY certain users can make the approval, irregardless of how the environment and pipeline is set up.Completing this circle, we can ensure that protected resources in Azure DevOps - environments AND service connections - extend from a particular template, ensuring compliance and standardization across the organization.Happy templating!Updates - Jan 5 2020 - Using Build Job template instead of Build Steps templateAfter using the template for a few weeks, I’ve made an update to be able to pass in build jobs instead of build steps. Note that with this template, you can pass in either. I prefer using a separate job under the build stage for the secret scanning bit so I can see what failed - the secret scan or the build.I have an input parameter for buildStageName so that this would still make sense in a scenario where there isn’t a stage named Build, such as in Terraform deployments. By default, the stage will be named build, but it can be optionally overridden (called something like secret-scanning instead, in the Terraform example).secret-scanning-extends.yml:parameters:- name: buildSteps # the name of the parameter is buildSteps type: stepList # data type is StepList default: [] # default value of buildSteps- name: buildJobs # the name of the parameter is buildSteps type: jobList # data type is StepList default: [] # default value of buildSteps- name: deployStages type: stageList default: [] - name: 'buildStageName' type: string default: 'build'resources: repositories: - repository: secretscanning type: github name: joshjohanning/secret-scanning-config endpoint: joshjohanningstages:- stage: ${{ parameters.buildStageName }} displayName: ${{ parameters.buildStageName }} jobs: - job: secret_scanning steps: - template: secret-scanning/secret-scanning-steps.yml - ${{ parameters.buildSteps }} - ${{ parameters.buildJobs }}- ${{ parameters.deployStages }}azure-pipelines.yml:trigger: - main resources: repositories: - repository: templates type: github name: joshjohanning/pipeline-templates endpoint: joshjohanningextends: template: secret-scanning/secret-scanning-extends.yml@templates parameters: buildJobs: # job template - template: my-build-job.yml@templates deployStages: - stage: dev displayName: deploy to dev jobs: # job template - template: secret-scanning/sample-deployment-job.yml@templates parameters: environment: github-secret-scanning-test-gate-dev - stage: prod displayName: deploy to prod jobs: - template: secret-scanning/sample-deployment-job.yml@templates parameters: environment: github-secret-scanning-test-gate-prod" } ]
